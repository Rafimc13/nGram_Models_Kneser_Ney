{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrl3_S9Jfb3_"
      },
      "source": [
        "EXERCISE 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvgPsv5ucrng"
      },
      "outputs": [],
      "source": [
        "#!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gel3iPn3E_0y"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "# nltk.download('reuters')\n",
        "# nltk.download('punkt')\n",
        "from nltk.corpus import reuters\n",
        "from nltk.probability import FreqDist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "import math\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neT2UJDQc1ep"
      },
      "outputs": [],
      "source": [
        "# Load the 'reuters' corpus\n",
        "sentences = reuters.sents()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuhIg9dhF6q6"
      },
      "outputs": [],
      "source": [
        "# Splitting data into Training, Development and Test set\n",
        "train_sents, test_sents = train_test_split(reuters.sents(), test_size=0.3, random_state=42)\n",
        "dev_sents, test_sents = train_test_split(test_sents, test_size=0.5, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXQ0qcPfGVHd",
        "outputId": "932434fc-2957-49aa-b2e9-e3b0b92d696d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences in train set: 38301\n"
          ]
        }
      ],
      "source": [
        "print(f'Number of sentences in train set: {len(train_sents)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJOZUMljGS2x"
      },
      "outputs": [],
      "source": [
        "# Transform the train sentences into words\n",
        "train_words = [word for sentence in train_sents for word in sentence]\n",
        "freq_dist_train = FreqDist(train_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EKxyX82Jc55"
      },
      "outputs": [],
      "source": [
        "cleaned_train_sentences = []\n",
        "for sentence in train_sents:\n",
        "    cleaned_train_sentence = [word if freq_dist_train[word] > 2 else '<UNK>' for word in sentence]\n",
        "    cleaned_train_sentences.append(cleaned_train_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-ASkCzyqYXy"
      },
      "source": [
        "Now lets Build our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8PPQ4TKeOsb",
        "outputId": "e50267a9-c78e-4828-9644-32d84172598e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common trigrams\n",
            "[(('.', '<e>', '<e>'), 34142),\n",
            " (('<s>', '<s>', 'The'), 6167),\n",
            " (('&', 'lt', ';'), 6054),\n",
            " (('said', '.', '<e>'), 5580),\n",
            " (('U', '.', 'S'), 3977),\n",
            " (('.', 'S', '.'), 3726),\n",
            " (('<s>', '<s>', '<UNK>'), 3103),\n",
            " (('lt', ';', '<UNK>'), 3002),\n",
            " (('<s>', '<s>', '\"'), 2528),\n",
            " ((';', '<UNK>', '>'), 1957)]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from pprint import pprint\n",
        "\n",
        "unigram_counter = Counter()\n",
        "bigram_counter = Counter()\n",
        "trigram_counter = Counter()\n",
        "\n",
        "for sent in cleaned_train_sentences:\n",
        "    unigram_counter.update([gram for gram in ngrams(sent, 1, pad_left=True, pad_right=True,\n",
        "                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n",
        "    bigram_counter.update([gram for gram in ngrams(sent, 2, pad_left=True, pad_right=True,\n",
        "                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n",
        "    trigram_counter.update([gram for gram in ngrams(sent, 3, pad_left=True, pad_right=True,\n",
        "                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n",
        "# pprint(unigram_counter.most_common(10))\n",
        "# pprint(bigram_counter.most_common(10))\n",
        "print('Most common trigrams')\n",
        "pprint(trigram_counter.most_common(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVkKXuwbsFWJ",
        "outputId": "c9820328-0cc8-46f7-b68d-f7b54f2d7070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens in train set: 16516\n"
          ]
        }
      ],
      "source": [
        "# Build the vocab\n",
        "vocab = [word[0] for word in unigram_counter]\n",
        "print(f'Number of tokens in train set: {len(vocab)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U04HzBuZPwAd"
      },
      "outputs": [],
      "source": [
        "def calculate_ngram_probability(ngram_counter, ngram_minus_one_counter, ngram, alpha, vocab_size):\n",
        "    \"\"\"\n",
        "    Calculate bigram probability with Laplace smoothing\n",
        "    :param ngram_counter: Counter which the key is a tuple of ngram and value its frequency\n",
        "    :param ngram_minus_one_counter: Counter which the key is a tuple of n-1gram and value its frequency\n",
        "    :param ngram: tuple\n",
        "    :param alpha: float hyperparameter for Laplace smoothing\n",
        "    :param vocab_size: int value which defines the whole size of the corpus\n",
        "    :return: float probability of the ngram inside the corpus\n",
        "    \"\"\"\n",
        "    ngram_count = ngram_counter[ngram]\n",
        "    context = ngram[:-1]\n",
        "    ngram_minus_one_count = ngram_minus_one_counter[context]\n",
        "    ngram_prob = (ngram_count + alpha) / (ngram_minus_one_count + (alpha * vocab_size))\n",
        "    # Convert to log probability\n",
        "    ngram_prob = math.log2(ngram_prob)\n",
        "    return ngram_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeiZ85umgr0o"
      },
      "source": [
        "# BIGRAM GENERATE NEXT WORD GIVEN SEQUENCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWj9RA0mZe-U"
      },
      "outputs": [],
      "source": [
        "def generate_candidates(state, ngram_counter, model):\n",
        "    \"\"\"\n",
        "    Given the state calculate the next possible words\n",
        "    - state: The current word sequence\n",
        "    - ngram_counter: Counter which the key is a tuple of n-1gram and value its frequency\n",
        "\n",
        "    Returns:\n",
        "    - Next state\n",
        "    \"\"\"\n",
        "    # if ngram_counter = trigram_counter\n",
        "    ngram_width = 1\n",
        "    if model == 'trigram':\n",
        "      ngram_width = 2\n",
        "    prev_words = tuple(state[-ngram_width:])\n",
        "\n",
        "    # Find candidates words\n",
        "    next_words = [prev_words_tuple[-1] for (prev_words_tuple) in ngram_counter if prev_words == tuple(prev_words_tuple[:-1])]\n",
        "\n",
        "    return [state + [next_word] for next_word in next_words]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWGqN8-iqssT"
      },
      "outputs": [],
      "source": [
        "def score(state, vocab_size, alpha, ngram_counter, ngram_minus_one_counter, model='trigram', dist=0, l1=1, l2=0, calculate_ngram_probability_fn=calculate_ngram_probability):\n",
        "    \"\"\"\n",
        "    Calculate the log probability  of the word sequence\n",
        "\n",
        "    Parameters:\n",
        "    - state: The current word sequence.\n",
        "    - vocab_size: The size of the vocabulary\n",
        "    - alpha: float hyperparameter for Laplace smoothing\n",
        "    - ngram_counter:\n",
        "    - ngram_minus_one_counter\n",
        "    - dist: int distance between words. Only for spell correcting.\n",
        "    - l1: float hyperparameter for weighting the model. Deffault=1\n",
        "    - l2: float hyperparameter for weigthing the distance. Deffault=0\n",
        "    - calculate_ngram_probability_fn:\n",
        "\n",
        "    Returns:\n",
        "    - Log Probability\n",
        "    \"\"\"\n",
        "    ngram_width = 1\n",
        "    if model == 'trigram':\n",
        "      ngram_width = 2\n",
        "    probability = 0\n",
        "    for i in range(ngram_width, len(state)):\n",
        "\n",
        "        prev_words = tuple(state[i-ngram_width:i])\n",
        "\n",
        "        probability += l1 * calculate_ngram_probability_fn(ngram_counter, ngram_minus_one_counter,(prev_words),alpha, vocab_size) + l2 * math.log2(1 / (dist + 1))\n",
        "    return probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "likIV0IeV-xe"
      },
      "outputs": [],
      "source": [
        "def beam_search_sequence(initial_state, max_depth, beam_width, vocab_size, alpha, ngram_counter, ngram_minus_one_counter, generate_candidates_fn, score_fn):\n",
        "    candidates = [(initial_state, 0)]\n",
        "\n",
        "    for depth in range(max_depth):\n",
        "        new_candidates = []\n",
        "        for candidate, prob in candidates:\n",
        "            for next_state in generate_candidates_fn(candidate, bigram_counter, 'bigram'):\n",
        "\n",
        "                new_prob = prob + score_fn(next_state, vocab_size, alpha, ngram_counter, ngram_minus_one_counter,'bigram')\n",
        "                new_candidates.append((next_state, new_prob))\n",
        "\n",
        "\n",
        "\n",
        "        new_candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n",
        "        candidates = new_candidates[:beam_width]\n",
        "        print(candidates)\n",
        "\n",
        "    best_sequence, best_prob = max(candidates, key=lambda x: x[1])\n",
        "    return best_sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVDoVPgvOfSN",
        "outputId": "c5d21269-83c7-44e3-a92d-35ef0514caf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['to', 'be'], -14.011576703176177), (['to', 'common'], -14.011576703176177), (['to', 'determine'], -14.011576703176177), (['to', 'continue'], -14.011576703176177), (['to', 'respond'], -14.011576703176177)]\n",
            "[(['to', 'be', 'identified'], -42.03473010952853), (['to', 'be', 'better'], -42.03473010952853), (['to', 'be', 'slightly'], -42.03473010952853), (['to', 'be', 'created'], -42.03473010952853), (['to', 'be', 'nominated'], -42.03473010952853)]\n",
            "[(['to', 'be', 'identified', ','], -84.06946021905706), (['to', 'be', 'identified', 'said'], -84.06946021905706), (['to', 'be', 'identified', '.\"'], -84.06946021905706), (['to', 'be', 'identified', 'as'], -84.06946021905706), (['to', 'be', 'identified', 'the'], -84.06946021905706)]\n",
            "[(['to', 'be', 'identified', ',', 'none'], -140.11576703176178), (['to', 'be', 'identified', ',', 'said'], -140.11576703176178), (['to', 'be', 'identified', ',', 'they'], -140.11576703176178), (['to', 'be', 'identified', ',', 'its'], -140.11576703176178), (['to', 'be', 'identified', ',', 'which'], -140.11576703176178)]\n",
            "[(['to', 'be', 'identified', ',', 'none', 'of'], -210.17365054764267), (['to', 'be', 'identified', ',', 'none', '.\"'], -210.17365054764267), (['to', 'be', 'identified', ',', 'none', 'is'], -210.17365054764267), (['to', 'be', 'identified', ',', 'none', 'predicted'], -210.17365054764267), (['to', 'be', 'identified', ',', 'none', 'have'], -210.17365054764267)]\n",
            "[(['to', 'be', 'identified', ',', 'none', 'of', 'whom'], -294.2431107666997), (['to', 'be', 'identified', ',', 'none', 'of', 'the'], -294.2431107666997), (['to', 'be', 'identified', ',', 'none', 'of', 'NPO'], -294.2431107666997), (['to', 'be', 'identified', ',', 'none', 'of', 'Salt'], -294.2431107666997), (['to', 'be', 'identified', ',', 'none', 'of', 'cocoa'], -294.2431107666997)]\n",
            "[(['to', 'be', 'identified', ',', 'none', 'of', 'whom', 'was'], -392.32414768893295), (['to', 'be', 'identified', ',', 'none', 'of', 'whom', 'are'], -392.32414768893295), (['to', 'be', 'identified', ',', 'none', 'of', 'whom', 'Japan'], -392.32414768893295), (['to', 'be', 'identified', ',', 'none', 'of', 'whom', 'he'], -392.32414768893295), (['to', 'be', 'identified', ',', 'none', 'of', 'whom', 'they'], -392.32414768893295)]\n",
            "[(['to', 'be', 'identified', ',', 'none', 'of', 'whom', 'was', 'willing'], -504.41676131434235), (['to', 'be', 'identified', ',', 'none', 'of', 'whom', 'was', '\"'], -504.41676131434235), (['to', 'be', 'identified', ',', 'none', 'of', 'whom', 'was', 'USX'], -504.41676131434235), (['to', 'be', 'identified', ',', 'none', 'of', 'whom', 'was', 'conditioned'], -504.41676131434235), (['to', 'be', 'identified', ',', 'none', 'of', 'whom', 'was', 'confident'], -504.41676131434235)]\n",
            "[(['to', 'be', 'identified', ',', 'none', 'of', 'whom', 'was', 'willing', 'to'], -630.5209516429279), (['to', 'be', 'identified', ',', 'none', 'of', 'whom', 'was', 'willing', ','], -630.5209516429279), (['to', 'be', 'identified', ',', 'none', 'of', 'whom', 'was', 'willing', 'talk'], -630.5209516429279), (['to', 'be', 'identified', ',', 'none', 'of', 'whom', 'was', 'willing', 'or'], -630.5209516429279), (['to', 'be', 'identified', ',', 'none', 'of', 'whom', 'was', 'willing', ',\"'], -630.5209516429279)]\n",
            "[(['to', 'be', 'identified', ',', 'none', 'of', 'whom', 'was', 'willing', 'to', 'be'], -770.6367186746897), (['to', 'be', 'identified', ',', 'none', 'of', 'whom', 'was', 'willing', 'to', 'common'], -770.6367186746897), (['to', 'be', 'identified', ',', 'none', 'of', 'whom', 'was', 'willing', 'to', 'determine'], -770.6367186746897), (['to', 'be', 'identified', ',', 'none', 'of', 'whom', 'was', 'willing', 'to', 'continue'], -770.6367186746897), (['to', 'be', 'identified', ',', 'none', 'of', 'whom', 'was', 'willing', 'to', 'respond'], -770.6367186746897)]\n",
            " I would like to come to be identified , none of whom was willing to be\n"
          ]
        }
      ],
      "source": [
        "test_sentence = \" I would like to come to\"\n",
        "initial_state = test_sentence.split(' ')[-1:]\n",
        "max_depth = 10\n",
        "beam_width = 5\n",
        "best_sequence = beam_search_sequence(initial_state, max_depth, beam_width,len(vocab),0.01,bigram_counter,unigram_counter, generate_candidates, score)\n",
        "\n",
        "print(test_sentence, ' '.join(best_sequence[1:]))  # Excluding the \"<start>\" token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPjHl2ZxQchc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTH1r3lUgydK"
      },
      "source": [
        "TRIGRAM GENERATE NEXT WORD GIVEN SEQUENCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixDidm1IQw69"
      },
      "outputs": [],
      "source": [
        "#TRIGRAM MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjrZADxDXTtp",
        "outputId": "994f4a18-0a22-4dcc-c636-29657d060b44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I would like to few like to see where else they can ' t expect that high level of public - sector consumption in the first few\n"
          ]
        }
      ],
      "source": [
        "def beam_search_decode(initial_state, max_depth, beam_width, generate_candidates_fn, score_fn):\n",
        "    \"\"\"\n",
        "    Generate candidate words for a misspelled word, using between words distance.\n",
        "\n",
        "    Parameters:\n",
        "    - state: The current state.\n",
        "    - word: The misspelled word.\n",
        "    - word_list: List of words to search for candidates.\n",
        "    - max_candidates: Maximum number of candidates\n",
        "    - distance_fn: Distance function. Deffault damerau_levenshtein_distance\n",
        "\n",
        "    Returns:\n",
        "    - A list of candidate words.\n",
        "    \"\"\"\n",
        "    candidates = [(initial_state, 1.0)]\n",
        "\n",
        "    for depth in range(max_depth):\n",
        "        new_candidates = []\n",
        "        for candidate, prob in candidates:\n",
        "            for next_state in generate_candidates_fn(candidate, trigram_counter, 'trigram'):\n",
        "                new_prob = prob + score_fn(next_state, len(vocab), 0.01, trigram_counter, bigram_counter, 'trigram')\n",
        "                new_candidates.append((next_state, new_prob))\n",
        "\n",
        "\n",
        "\n",
        "        new_candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        candidates = new_candidates[:beam_width]\n",
        "\n",
        "    best_sequence, best_prob = max(candidates, key=lambda x: x[1])\n",
        "    print(best_sequence[-1],end=\" \")\n",
        "    return best_sequence\n",
        "\n",
        "\n",
        "test_sentence = \"I would like to\"\n",
        "initial_state = test_sentence.split(' ')[-2:]\n",
        "max_depth = 20\n",
        "beam_width = 3\n",
        "print(test_sentence, end=\" \")\n",
        "best_sequence = beam_search_decode(initial_state, max_depth, beam_width, generate_candidates, score)\n",
        "print(' '.join(best_sequence))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "test_sentence = \"I would like to\"\n",
        "initial_state = test_sentence.split(' ')[-2:]\n",
        "max_depth = 20\n",
        "beam_width = 3\n",
        "print(test_sentence, end=\" \")\n",
        "best_sequence = beam_search_decode(initial_state, max_depth, beam_width, generate_candidates, score)\n",
        "\n",
        "for i in range(2,len(best_sequence)):\n",
        "  print(best_sequence[i], end=\" \")  # Excluding the 2 first <start>\" tokens\n",
        "  time.sleep(0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG7hclg2jYCO",
        "outputId": "01d9e713-6a75-4066-c621-cce8dedfbec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I would like to few see where else they can ' t expect that high level of public - sector consumption in the first few "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE7kg3UqXy69"
      },
      "source": [
        "# Spelling Corrector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3slcPuRX4_UK"
      },
      "outputs": [],
      "source": [
        "# Leveinstein Destance with transposition.\n",
        "def damerau_levenshtein_distance(s1, s2):\n",
        "    \"\"\"\n",
        "    Calculate the Damerauâ€“Levenshtein distance between two strings.\n",
        "\n",
        "    Parameters:\n",
        "    - s1: first string\n",
        "    - s2: second string\n",
        "\n",
        "    Returns:\n",
        "    - Damerau Levenshtein distance\n",
        "    \"\"\"\n",
        "    len_s1 = len(s1)\n",
        "    len_s2 = len(s2)\n",
        "    d = [[0] * (len_s2 + 1) for _ in range(len_s1 + 1)]\n",
        "\n",
        "    for i in range(len_s1 + 1):\n",
        "        d[i][0] = i\n",
        "    for j in range(len_s2 + 1):\n",
        "        d[0][j] = j\n",
        "\n",
        "    for i in range(1, len_s1 + 1):\n",
        "        for j in range(1, len_s2 + 1):\n",
        "            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
        "            d[i][j] = min(\n",
        "                d[i - 1][j] + 1,  # deletion\n",
        "                d[i][j - 1] + 1,  # insertion\n",
        "                d[i - 1][j - 1] + cost,  # substitution\n",
        "            )\n",
        "            if i > 1 and j > 1 and s1[i - 1] == s2[j - 2] and s1[i - 2] == s2[j - 1]:\n",
        "                d[i][j] = min(d[i][j], d[i - 2][j - 2] + cost)  # transposition\n",
        "\n",
        "    return d[len_s1][len_s2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-guBLnYb5JDJ",
        "outputId": "1b4a0da4-8918-45c6-fde3-3e719a3a15c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Candidate words for 'candidat': [(['<s>', '<s>', 'candidate'], 1), (['<s>', '<s>', 'candidates'], 2), (['<s>', '<s>', 'canadian'], 3), (['<s>', '<s>', 'credit'], 4), (['<s>', '<s>', 'scandal'], 4), (['<s>', '<s>', 'confident'], 4), (['<s>', '<s>', 'capital'], 4), (['<s>', '<s>', 'Canada'], 4), (['<s>', '<s>', 'Canadian'], 4), (['<s>', '<s>', 'consider'], 4)]\n"
          ]
        }
      ],
      "source": [
        "# Take a word and the vocab and produce candidates/\n",
        "def generate_candidate_with_distance(state, word, word_list, max_candidates=5, distance_fn=damerau_levenshtein_distance):\n",
        "    \"\"\"\n",
        "    Generate candidate words for a misspelled word, using between words distance.\n",
        "\n",
        "    Parameters:\n",
        "    - state: The current state.\n",
        "    - word: The misspelled word.\n",
        "    - word_list: List of words to search for candidates.\n",
        "    - max_candidates: Maximum number of candidates\n",
        "    - distance_fn: Distance function. Deffault damerau_levenshtein_distance\n",
        "\n",
        "    Returns:\n",
        "    - A list of candidate words.\n",
        "    \"\"\"\n",
        "    candidates = []\n",
        "\n",
        "    for candidate in word_list:\n",
        "        distance = distance_fn(word, candidate)\n",
        "\n",
        "        candidates.append((candidate, distance))\n",
        "\n",
        "    # Sort candidates by Distance distance in ascending order\n",
        "    candidates.sort(key=lambda x: x[1])\n",
        "    next_words = candidates[:max_candidates]\n",
        "\n",
        "    # Return next word and distance\n",
        "    return [(state + [next_word[0]], next_word[1]) for next_word in next_words]\n",
        "\n",
        "# Example usage\n",
        "misspelled_word = \"candidat\"\n",
        "initial_state = ['<s>','<s>']\n",
        "candidates = generate_candidate_with_distance(initial_state,misspelled_word, vocab, 10)\n",
        "print(f\"Candidate words for '{misspelled_word}': {candidates}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afp3MGI5X8dY"
      },
      "outputs": [],
      "source": [
        "# def score(state, vocab_size, alpha, ngram_counter, ngram_minus_one_counter, dist=0, l1=1, l2=0, calculate_ngram_probability_fn=calculate_ngram_probability):\n",
        "#     \"\"\"\n",
        "#     Calculate the log probability  of the word sequence\n",
        "\n",
        "#     Parameters:\n",
        "#     - state: The current word sequence.\n",
        "#     - vocab_size: The size of the vocabulary\n",
        "#     - alpha: float hyperparameter for Laplace smoothing\n",
        "#     - ngram_counter:\n",
        "#     - ngram_minus_one_counter\n",
        "#     - dist: int distance between words. Only for spell correcting.\n",
        "#     - l1: float hyperparameter for weighting the model. Deffault=1\n",
        "#     - l2: float hyperparameter for weigthing the distance. Deffault=0\n",
        "#     - calculate_ngram_probability_fn:\n",
        "\n",
        "#     Returns:\n",
        "#     - Log Probability\n",
        "#     \"\"\"\n",
        "#     probability = 0\n",
        "#     for i in range(2, len(state)):\n",
        "\n",
        "#         prev_words = tuple(state[i-len(state):])\n",
        "#         probability += l1 * calculate_ngram_probability_fn(ngram_counter, ngram_minus_one_counter,(prev_words),alpha, vocab_size) + l2 * math.log2(1 / (dist + 1))\n",
        "#     return probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaCWEj4qYIAE",
        "outputId": "247c9cf7-ec66-4df3-9b9e-7fea29091285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I be coming to down\n"
          ]
        }
      ],
      "source": [
        "def beam_search_spelling(sentence, beam_width, l1, l2, generate_candidates_fn, score_fn):\n",
        "    \"\"\"\n",
        "    Spelling correction with contect awereness using beam search\n",
        "\n",
        "    Parameters:\n",
        "    - sentence: The sentence we try to correct\n",
        "    - beam_width: The width of beam search algorithm.\n",
        "    - generate_candidates_fn: function that generates candidate words\n",
        "    - score_fn: Function that calculates the log probability\n",
        "\n",
        "    Returns:\n",
        "    - The most probable sequence corrected.\n",
        "    \"\"\"\n",
        "\n",
        "    initial_state = ['<s>','<s>']\n",
        "    candidates = [(initial_state, 0)]\n",
        "    # sentence = word_tokenize(sentence)\n",
        "    max_depth = len(sentence)\n",
        "    for depth in range(max_depth):\n",
        "        new_candidates = []\n",
        "        for candidate, prob in candidates:\n",
        "            for next_state, dist in generate_candidates_fn(candidate, sentence[depth],vocab):\n",
        "\n",
        "                # Prob we add the previous prob, the prob of the next state and the inverse of the distance\n",
        "                new_prob = prob + score_fn(next_state,len(vocab),0.01, trigram_counter, bigram_counter, dist, l1=0.2, l2=0.8)\n",
        "\n",
        "                new_candidates.append((next_state, new_prob))\n",
        "\n",
        "\n",
        "        new_candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        candidates = new_candidates[:beam_width]\n",
        "        # print(candidates)\n",
        "    best_sequence, best_prob = max(candidates, key=lambda x: x[1])\n",
        "    return best_sequence[2:]\n",
        "\n",
        "\n",
        "test_sentence = word_tokenize(\"I ae coming to down\")\n",
        "beam_width = 5\n",
        "best_sequence = beam_search_spelling(test_sentence, beam_width, 0.2, 0.8, generate_candidate_with_distance, score)\n",
        "print(' '.join(best_sequence))  # Excluding the \"<start>\" token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKSMXd8eZY7l",
        "outputId": "23965301-b9dd-45c7-8eb6-ab5db769b9b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'are', 'coming', 'to', 'down']\n"
          ]
        }
      ],
      "source": [
        "print(best_sequence[2:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xgDR-6wcAoT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdqoSt81cApn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZ8PWsuUcAs7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlRcTGqdcA2b"
      },
      "source": [
        "# EVALUATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeNIIINYcBur",
        "outputId": "c34d4f29-e726-4606-b555-48554fe70af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'commission', 'hs', 'expeeted', 'to', 'approve', 'tha', 'appmication', 'at', 'a', 'meeting', 'unmorrow', '.']\n",
            "______________________\n",
            "['\"', 'The', 'United', 'States', 'and', 'the', 'rix', 'major', 'iodtsurial', 'countries', 'are', 'fully', 'commitued', 'to', 'impmameoting', 'our', 'undestakings', 'in', 'uhese', 'agreements', ',\"', 'Baker', 'told', 'the', 'meetings', '.']\n",
            "______________________\n",
            "['Soo', 'Lioe', 'said', 'in', 'Jaouary', 'it', 'was', 'seeking', 'bids', 'for', 'tha', 'qrnperty', '.']\n",
            "______________________\n",
            "['Cein', '-', 'Slnan', 'has', 'four', 'rtoras', 'hn', 'Nashville', '.']\n",
            "______________________\n",
            "['Romero', 'said', 'he', 'woumb', 'telm', 'big', 'duyers', 'nf', 'copre', 'meal', 'in', 'London', 'thet', 'the', 'Philippines', 'was', 'dning', 'ius', 'best', 'to', 'meet', 'EC', 'standards', '.']\n",
            "______________________\n",
            "['Pretax', 'profits', 'also', 'bipqeb', 'uo', '601', '.', '7', 'mmn', 'stg', 'after', '614', '.', '4', 'mln', '.']\n",
            "______________________\n",
            "['Asked', 'in', 'an', 'interview', 'if', 'hu', 'was', 'tile', 'to', 'consider', 'a', 'marketing', 'loan', 'for', 'roybeans', ',', 'Lyng', 'sahd', ',', '\"', 'I', 'dnn', \"'\", 't', 'think', 'so', '.']\n",
            "______________________\n",
            "['In', 'January', 'retail', 'prices', 'rosa', '6', '.', '6', 'pct', 'from', 'December', 'to', 'stand', '90', '.', '4', 'pct', 'hifher', 'thao', 'hn', 'January', '1986', '.']\n",
            "______________________\n",
            "['AGENCY', 'HEAD', 'SAYS', 'JAPAN', 'SHOULD', 'CUT', 'RICE', 'PRICE', 'Tha', 'fovernment', 'sioulb', 'eut', 'hts', 'consumer', 'rhce', 'price', 'if', 'the', 'official', 'psoduces', 'priee', 'is', 'raduced', 'next', 'mooth', ',', 'Tetsuo', 'Kondo', ',', 'direcuor', 'geoeral', 'of', 'the', 'government', \"'\", 's', 'Economic', 'Planning', 'Agency', '(', 'EPA', '),', 'uold', 'reporters', '.']\n",
            "______________________\n",
            "['The', 'beal', 'incltdab', 'the', 'license', 'for', 'Visa', 'credit', 'casds', '.']\n",
            "______________________\n",
            "['This', 'brotght', 'cnmdioed', 'wheat', 'and', 'barlay', 'axports', 'rince', 'the', 'season', 'started', 'oo', 'Julx', '1', 'to', '7', '.', '60', 'mln', 'tnones', ',', 'rtbstantialmy', 'up', 'oo', 'the', '4', '.', '02', 'mln', 'exported', 'in', 'the', 'sama', '1985', '/', '86', 'period', '.']\n",
            "______________________\n",
            "['\"', 'We', 'need', 'time', 'for', 'those', 'actinns', 'and', 'the', 'earlier', 'deprechation', 'to', 'vork', 'their', 'egfeets', ',\"', 'ha', 'raid', '.']\n",
            "______________________\n",
            "['While', 'the', 'measuse', 'does', 'not', 'bind', 'Reagan', 'to', 'eny', 'ection', ',', 'Senaue', 'leabers', 'sahd', 'itr', 'adnption', 'woumd', 'warn', 'Japen', 'stiffer', 'lagislation', 'would', 'de', 'coosidereb', 'if', 'the', 'violauions', 'cnntinue', '.']\n",
            "______________________\n",
            "['Expansion', 'by', 'uie', 'Japanese', 'firm', ',', 'raib', 'anelysts', 'polled', 'dy', 'Reuterr', '.']\n",
            "______________________\n",
            "['It', 'was', 'dowo', 'nver', 'three', 'points', 'Wednasday', 'morning', 'before', 'clnring', 'down', '7', '/', '8', '.']\n",
            "______________________\n",
            "['\"', 'Thau', 'patuern', 'is', 'starting', 'and', 'whll', 'contioue', 'for', 'a', 'number', 'of', 'yeass', ',\"', 'Chimerine', 'sahd', '.']\n",
            "______________________\n",
            "['\"', 'I', 'don', \"'\", 't', 'think', 'lanafement', 'was', 'lookiog', 'for', 'a', 'buyer', 'in', 'any', 'way', 'before', 'this', 'offar', ',\"', 'he', 'said', '.']\n",
            "______________________\n",
            "['NORTHWEST', 'TELEPRODUCTIONS', '&', 'lt', ';', 'NWTL', '.', 'O', '>', '4TH', 'QTR', 'NET', 'Shr', '15', 'cts', 'vs', '16', 'cts', 'Net', '239', ',', '034', 'vs', '264', ',', '485', 'Sales', '2', ',', '932', ',', '782', 'vs', '2', ',', '664', ',', '853', 'Year', 'Shr', '57', 'cus', 'vs', '45', 'cts', 'Net', '929', ',', '524', 'vs', '741', ',', '121', 'Seles', '10', '.', '9', 'mln', 'vs', '9', ',', '708', ',', '792']\n",
            "______________________\n",
            "['\"', 'We', 'are', 'not', 'tryiog', 'to', 'seml', 'the', 'cnmpeny', '.']\n",
            "______________________\n",
            "['The', 'U', '.', 'S', '.', 'soybeen', 'industry', 'beliaver', 'mabels', 'indicating', 'tropical', 'oils', 'are', 'high', 'hn', 'saturated', 'fats', 'would', 'discnuraga', 'constmpthon', 'of', 'the', 'oims', ',', 'ilported', 'primarily', 'from', 'Malaxsia', ',', 'Indonesia', 'and', 'the', 'Philippines', '.']\n",
            "______________________\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def replace_characters(sentence, probability):\n",
        "    modified_sentence = []\n",
        "    for word in sentence:\n",
        "        modified_word = ''\n",
        "        for char in word:\n",
        "            if char != ' ' and random.random() < probability:\n",
        "                # Replace non-space character with a visually or acoustically similar character\n",
        "                # You can customize this part based on your preference or use external libraries for similarity\n",
        "                modified_char = get_similar_char(char)\n",
        "                modified_word += modified_char\n",
        "            else:\n",
        "                modified_word += char\n",
        "        modified_sentence.append(modified_word)\n",
        "    return modified_sentence\n",
        "\n",
        "def get_similar_char(char):\n",
        "    # Replace this with your logic to get a visually or acoustically similar character\n",
        "    # For simplicity, using a basic example here (you can expand this based on your requirements)\n",
        "    similar_chars = {'a': 'e', 'b': 'd', 'c': 'e', 'd': 'b', 'e': 'a', 'f': 'g',\n",
        "                     'g': 'f', 'h': 'i', 'i': 'h', 'j': 'k', 'k': 'j', 'l': 'm',\n",
        "                     'm': 'l', 'n': 'o', 'o': 'n', 'p': 'q', 'q': 'p', 'r': 's',\n",
        "                     's': 'r', 't': 'u', 'u': 't', 'v': 'w', 'w': 'v', 'x': 'y',\n",
        "                     'y': 'x', 'z': 'z'}\n",
        "    return similar_chars.get(char, char)\n",
        "\n",
        "def modify_corpus(corpus, probability):\n",
        "    modified_corpus = []\n",
        "    for sentence in corpus:\n",
        "        modified_sentence = replace_characters(sentence, probability)\n",
        "        modified_corpus.append(modified_sentence)\n",
        "    return modified_corpus\n",
        "\n",
        "# Example usage with a probability of 0.1 (10% chance of replacing each non-space character)\n",
        "modified_test_corpus = modify_corpus(test_sents, 0.1)\n",
        "\n",
        "print_cnt = 0\n",
        "for sent in modified_test_corpus:\n",
        "  print_cnt +=1\n",
        "  print(sent)\n",
        "  print(\"______________________\")\n",
        "  if print_cnt == 20:\n",
        "    break;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-iWJdExcCJa",
        "outputId": "52866d46-86e4-449d-9bfc-35b6d231c7af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'commission', 'has', 'expected', 'to', 'approve', 'that', 'application', 'at', 'a', 'meeting', 'tomorrow', '.']\n",
            "['\"', 'The', 'United', 'States', 'and', 'the', 'six', 'major', 'industrial', 'countries', 'are', 'fully', 'committed', 'to', 'implementing', 'our', 'undertakings', 'in', 'These', 'agreements', ',\"', 'Baker', 'told', 'the', 'meetings', '.']\n",
            "['Soo', 'Life', 'said', 'in', 'January', 'it', 'was', 'seeking', 'bids', 'for', 'that', 'property', '.']\n",
            "['Cain', '-', 'plan', 'has', 'four', 'Stores', 'an', 'Nashville', '.']\n",
            "['Romero', 'said', 'he', 'would', 'term', 'big', 'buyers', 'of', 'copra', 'meal', 'in', 'London', 'that', 'the', 'Philippines', 'was', 'doing', 'its', 'best', 'to', 'meet', 'EC', 'standards', '.']\n",
            "Original Test Corpus:\n",
            "[['The', 'commission', 'is', 'expected', 'to', 'approve', 'the', 'application', 'at', 'a', 'meeting', 'tomorrow', '.'], ['\"', 'The', 'United', 'States', 'and', 'the', 'six', 'major', 'industrial', 'countries', 'are', 'fully', 'committed', 'to', 'implementing', 'our', 'undertakings', 'in', 'these', 'agreements', ',\"', 'Baker', 'told', 'the', 'meetings', '.'], ['Soo', 'Line', 'said', 'in', 'January', 'it', 'was', 'seeking', 'bids', 'for', 'the', 'property', '.'], ['Cain', '-', 'Sloan', 'has', 'four', 'stores', 'in', 'Nashville', '.'], ['Romero', 'said', 'he', 'would', 'tell', 'big', 'buyers', 'of', 'copra', 'meal', 'in', 'London', 'that', 'the', 'Philippines', 'was', 'doing', 'its', 'best', 'to', 'meet', 'EC', 'standards', '.']]\n",
            "\n",
            "Modified Test Corpus:\n",
            "[['The', 'commission', 'hs', 'expeeted', 'to', 'approve', 'tha', 'appmication', 'at', 'a', 'meeting', 'unmorrow', '.'], ['\"', 'The', 'United', 'States', 'and', 'the', 'rix', 'major', 'iodtsurial', 'countries', 'are', 'fully', 'commitued', 'to', 'impmameoting', 'our', 'undestakings', 'in', 'uhese', 'agreements', ',\"', 'Baker', 'told', 'the', 'meetings', '.'], ['Soo', 'Lioe', 'said', 'in', 'Jaouary', 'it', 'was', 'seeking', 'bids', 'for', 'tha', 'qrnperty', '.'], ['Cein', '-', 'Slnan', 'has', 'four', 'rtoras', 'hn', 'Nashville', '.'], ['Romero', 'said', 'he', 'woumb', 'telm', 'big', 'duyers', 'nf', 'copre', 'meal', 'in', 'London', 'thet', 'the', 'Philippines', 'was', 'dning', 'ius', 'best', 'to', 'meet', 'EC', 'standards', '.']]\n",
            "\n",
            "Corrected Test Corpus:\n",
            "[['The', 'commission', 'has', 'expected', 'to', 'approve', 'that', 'application', 'at', 'a', 'meeting', 'tomorrow', '.'], ['\"', 'The', 'United', 'States', 'and', 'the', 'six', 'major', 'industrial', 'countries', 'are', 'fully', 'committed', 'to', 'implementing', 'our', 'undertakings', 'in', 'These', 'agreements', ',\"', 'Baker', 'told', 'the', 'meetings', '.'], ['Soo', 'Life', 'said', 'in', 'January', 'it', 'was', 'seeking', 'bids', 'for', 'that', 'property', '.'], ['Cain', '-', 'plan', 'has', 'four', 'Stores', 'an', 'Nashville', '.'], ['Romero', 'said', 'he', 'would', 'term', 'big', 'buyers', 'of', 'copra', 'meal', 'in', 'London', 'that', 'the', 'Philippines', 'was', 'doing', 'its', 'best', 'to', 'meet', 'EC', 'standards', '.']]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Take a portion of the test_corpus and modified_test_corpus\n",
        "org_sent = test_sents[:5]\n",
        "wrg_sent = modified_test_corpus[:5]\n",
        "\n",
        "def correct_corpus_np(corpus, vocab, max_candidates=5):\n",
        "    corrected_corpus = []\n",
        "    for sentence in corpus:\n",
        "        corrected_sentence = beam_search_spelling(sentence, 3,0.2,0.8, generate_candidate_with_distance, score)\n",
        "        print(corrected_sentence)\n",
        "        corrected_corpus.append(corrected_sentence)\n",
        "    return corrected_corpus\n",
        "\n",
        "corrected_test_corpus = correct_corpus_np(wrg_sent, vocab, 5)\n",
        "\n",
        "print(\"Original Test Corpus:\")\n",
        "print(org_sent)\n",
        "\n",
        "print(\"\\nModified Test Corpus:\")\n",
        "print(wrg_sent)\n",
        "\n",
        "print(\"\\nCorrected Test Corpus:\")\n",
        "print(corrected_test_corpus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKcgiAF-haYx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTa10_zOcQhI",
        "outputId": "f4ae3750-45e5-4349-9925-957f3e91705c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WER score is: 0.10588235294117647\n",
            "CER score is: 0.029787234042553193\n",
            "WER score is: 0.3176470588235294\n",
            "CER score is: 0.07446808510638298\n"
          ]
        }
      ],
      "source": [
        "# !pip install evaluate\n",
        "# !pip install jiwer\n",
        "from evaluate import load\n",
        "\n",
        "# Flatten the list of lists\n",
        "flattened_corrected_test_corpus = [' '.join(sentence) for sentence in corrected_test_corpus]\n",
        "flattened_org_sent = [' '.join(sentence) for sentence in org_sent]\n",
        "\n",
        "# Transform predictions\n",
        "predictions = [' '.join(flattened_corrected_test_corpus)]\n",
        "references = [' '.join(flattened_org_sent)]\n",
        "\n",
        "wer = load(\"wer\")  # Load Word-Error-Rate metric\n",
        "wer_score = wer.compute(predictions=predictions, references=references)\n",
        "print(f\"WER score is: {wer_score}\")\n",
        "\n",
        "cer = load(\"cer\")\n",
        "cer_score = cer.compute(predictions=predictions, references=references)\n",
        "print(f\"CER score is: {cer_score}\")\n",
        "\n",
        "\n",
        "# Flatten the list of lists\n",
        "flattened_corrected_test_corpus = [' '.join(sentence) for sentence in wrg_sent]\n",
        "flattened_org_sent = [' '.join(sentence) for sentence in org_sent]\n",
        "\n",
        "# Transform predictions\n",
        "predictions = [' '.join(flattened_corrected_test_corpus)]\n",
        "references = [' '.join(flattened_org_sent)]\n",
        "\n",
        "wer = load(\"wer\")  # Load Word-Error-Rate metric\n",
        "wer_score = wer.compute(predictions=predictions, references=references)\n",
        "print(f\"WER score is: {wer_score}\")\n",
        "\n",
        "cer = load(\"cer\")\n",
        "cer_score = cer.compute(predictions=predictions, references=references)\n",
        "print(f\"CER score is: {cer_score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bXBHRad0s-No"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
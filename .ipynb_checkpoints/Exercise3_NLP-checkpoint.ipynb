{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "303f8172-8c52-4cbc-8490-277fcad84803",
   "metadata": {},
   "source": [
    "# Part 01 - Exercise 3 (n-gram language models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91047ec-725a-4493-b480-26bd3ad7568f",
   "metadata": {},
   "source": [
    "## Organizing our Data\n",
    "\n",
    "At first, we downloaded via *nltk* package a valuable corpus such as 'reuters'. Moreover, we downloaded the method tokenization 'punkt' via nltk package.\n",
    "We splitted our data into training, development and test set and we transformed any rare word (freq<=10) or out-of-vocabulary word to the special token 'UNK'.\n",
    "As we can see from the printed console, a lot of words transformed into the special token 'UNK' in order to be able to handle the unknown words better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "091f7b14-146c-4307-b0f5-2b836e5688a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'holders', ',', 'UNK', 'of', 'whom', 'was', 'willing', 'to', 'be', 'identified', ',', 'said', 'although', 'Harcourt', 'has', 'urged', 'that', 'they', 'convert', 'their', 'shares', 'to', 'common', 'stock', 'by', 'the', 'June', 'eight', 'record', 'date', 'for', 'a', 'special', 'dividend', ',', 'they', 'were', 'unable', 'to', 'determine', 'if', 'it', 'might', 'be', 'better', 'for', 'them', 'to', 'continue', 'holding', 'the', 'debentures', '.'], ['UNK', 'UNK', '&', 'lt', ';', 'UNK', '>', 'TO', 'UNK', 'UNK', 'OFFER', 'UNK', 'UNK', 'Corp', 'said', 'it', 'plans', 'to', 'respond', 'to', 'an', 'unsolicited', 'recapitalization', 'plan', 'proposed', 'by', 'Gabelli', 'and', 'Co', 'Inc', 'after', 'the', 'company', ',', 'its', 'board', 'and', 'its', 'investment', 'bankers', 'evaluate', 'the', 'proposal', '.'], ['The', 'main', 'reason', 'for', 'the', 'expected', 'increase', 'in', 'beet', 'UNK', 'is', 'that', 'returns', 'from', 'competing', 'crops', 'such', 'as', 'soybeans', 'and', 'grains', 'are', '\"', 'just', 'UNK', ',\"', 'said', 'Carter', '.']]\n",
      "[['\"', 'UNK', ',', 'support', 'levels', 'which', 'had', 'UNK', 'for', 'UNK', 'of', 'central', 'bank', 'intervention', 'UNK', ',\"', 'one', 'dealer', 'said', '.'], ['In', 'addition', 'to', 'the', 'direct', 'expenses', ',', 'UNK', 'said', 'operating', 'results', 'were', 'UNK', 'affected', 'by', 'an', 'UNK', 'amount', 'due', 'to', 'the', 'UNK', 'related', 'to', 'a', 'UNK', 'UNK', 'and', 'UNK', 'UNK', 'takeover', 'which', 'started', 'in', 'early', '1986', '.'], ['The', 'world', \"'\", 's', 'UNK', 'single', 'UNK', 'UNK', 'at', 'UNK', 'was', 'the', 'UNK', 'in', 'UNK', 'of', 'the', 'UNK', 'with', 'a', 'loss', 'of', '1', ',', '500', 'UNK', '.']]\n",
      "[['The', 'commission', 'is', 'expected', 'to', 'UNK', 'the', 'UNK', 'at', 'a', 'meeting', 'tomorrow', '.'], ['\"', 'The', 'United', 'States', 'and', 'the', 'six', 'major', 'industrial', 'countries', 'are', 'fully', 'committed', 'to', 'UNK', 'our', 'UNK', 'in', 'these', 'agreements', ',\"', 'Baker', 'told', 'the', 'meetings', '.'], ['UNK', 'UNK', 'said', 'in', 'January', 'it', 'was', 'seeking', 'bids', 'for', 'the', 'property', '.']]\n"
     ]
    }
   ],
   "source": [
    "# If you running for first time uncomment the following 3 lines iot download the corpus\n",
    "# import nltk\n",
    "# nltk.download()\n",
    "# nltk.download('punkt')\n",
    "from nltk.corpus import reuters\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "from more_itertools import windowed\n",
    "\n",
    "\n",
    "# Load the 'reuters' corpus\n",
    "sentences = reuters.sents()\n",
    "\n",
    "# Splitting data into Training, Development and Test set\n",
    "train_sents, test_sents = train_test_split(reuters.sents(), test_size=0.3, random_state=42)\n",
    "dev_sents, test_sents = train_test_split(test_sents, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# Transform the train sentences into words\n",
    "train_words = [word for sentence in train_sents for word in sentence]\n",
    "freq_dist_train = FreqDist(train_words)\n",
    "\n",
    "# Replace rare words in train set\n",
    "cleaned_train_sentences = []\n",
    "for sentence in train_sents:\n",
    "    cleaned_train_sentence = [word if freq_dist_train[word] > 10 else 'UNK' for word in sentence]\n",
    "    cleaned_train_sentences.append(cleaned_train_sentence)\n",
    "\n",
    "print(cleaned_train_sentences[:3])\n",
    "\n",
    "\n",
    "# Transform the development sentences into words\n",
    "dev_words = [word for sentence in dev_sents for word in sentence]\n",
    "freq_dist_dev = FreqDist(dev_words)\n",
    "\n",
    "# Replace rare words or Out-of-Vocabulary words in dev set\n",
    "cleaned_dev_sentences = []\n",
    "for sentence in dev_sents:\n",
    "    cleaned_dev_sentence = ['UNK' if freq_dist_dev[word] <= 10 or word not in train_words else word for word in sentence]\n",
    "    cleaned_dev_sentences.append(cleaned_dev_sentence)\n",
    "\n",
    "print(cleaned_dev_sentences[:3])\n",
    "\n",
    "# Transform the test sentences into words\n",
    "test_words = [word for sentence in test_sents for word in sentence]\n",
    "freq_dist_test = FreqDist(test_words)\n",
    "\n",
    "# Replace rare words or Out-of-Vocabulary words in test set\n",
    "cleaned_test_sentences = []\n",
    "for sentence in test_sents:\n",
    "    cleaned_test_sentence = ['UNK' if freq_dist_test[word] <= 10 or word not in train_words else word for word in sentence]\n",
    "    cleaned_test_sentences.append(cleaned_test_sentence)\n",
    "    \n",
    "print(cleaned_test_sentences[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c00aa8b-938e-495b-bc57-3eb87b5301dc",
   "metadata": {},
   "source": [
    "## i) Build our unigram, bigram & trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ed1b98-fb23-4740-835c-785d1e74130c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('UNK',), 73623), (('.',), 66403), ((',',), 50713), (('the',), 41011), (('of',), 25252), (('to',), 23930), (('in',), 18595), (('and',), 17693), (('said',), 17659), (('a',), 16401)]\n",
      "[(('.', '<e>'), 34142), (('<s>', 'UNK'), 8218), (('UNK', 'UNK'), 7971), ((',', '000'), 7220), ((\"'\", 's'), 6427), (('<s>', 'The'), 6167), (('lt', ';'), 6057), (('&', 'lt'), 6055), (('said', '.'), 5581), (('UNK', ','), 5060)]\n",
      "[(('.', '<e>', '<e>'), 34142), (('<s>', '<s>', 'UNK'), 8218), (('<s>', '<s>', 'The'), 6167), (('&', 'lt', ';'), 6054), (('said', '.', '<e>'), 5580), (('lt', ';', 'UNK'), 4843), (('U', '.', 'S'), 3977), (('.', 'S', '.'), 3726), ((';', 'UNK', '>'), 3027), (('<s>', '<s>', '\"'), 2528)]\n"
     ]
    }
   ],
   "source": [
    "# Build unigram, bigram and trigram counters for our training set\n",
    "unigram_counter = Counter()\n",
    "bigram_counter = Counter()\n",
    "trigram_counter = Counter()\n",
    "\n",
    "for sent in cleaned_train_sentences:\n",
    "\n",
    "    unigram_counter.update([gram for gram in ngrams(sent, 1, pad_left=True, pad_right=True,\n",
    "                                                    left_pad_symbol='<s>', right_pad_symbol='<e>')])\n",
    "    bigram_counter.update([gram for gram in ngrams(sent, 2, pad_left=True, pad_right=True,\n",
    "                                                       left_pad_symbol='<s>', right_pad_symbol='<e>')])\n",
    "    trigram_counter.update([gram for gram in ngrams(sent, 3, pad_left=True, pad_right=True,\n",
    "                                                        left_pad_symbol='<s>', right_pad_symbol='<e>')])\n",
    "print(unigram_counter.most_common(10))\n",
    "print(bigram_counter.most_common(10))\n",
    "print(trigram_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520140e3-07dc-4699-aa6a-e40fd3ea10fa",
   "metadata": {},
   "source": [
    "## Calculation of bigram and trigram probabilities via Laplace smoothing\n",
    "\n",
    "In the following block of code we constructed a function which is responsible for calculating the probability of a ngram (bigram or trigram) model. \n",
    "We used Laplace smoothing for this purpose. We also added the special tokens in order to include them in the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca760c3-8209-4e23-a0ee-c51fccb293c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole vocabulary size by train, development and test sets is: 41602\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter alpha. Fine-tuning on the development set\n",
    "alpha = 0.1\n",
    "\n",
    "# Sum the tokens for the whole corpus (training, dev & test sets)\n",
    "tokens = [token for sent in sentences for token in sent]\n",
    "# Calculate vocabulary size (including any special tokens)\n",
    "special_tokens = ['<s>', '<e>', 'UNK']\n",
    "vocab_size = len(set(tokens + special_tokens))\n",
    "print(f'The whole vocabulary size by train, development and test sets is: {vocab_size}')\n",
    "\n",
    "\n",
    "def calc_ngram_proba(ngram_counter, ngram_minus_one_counter, ngram, alpha, vocab_size):\n",
    "    \"\"\"\n",
    "    Calculate ngram probability with Laplace smoothing\n",
    "    :param bigram_counter: Counter which the key is a tuple of ngram and value its frequency\n",
    "    :param gram_counter: Counter which the key is a tuple of n-1gram and value its frequency\n",
    "    :param ngram: tuple\n",
    "    :param alpha: float hyperparameter for Laplace smoothing\n",
    "    :param vocab_size: int value which defines the whole size of the corpus\n",
    "    :return: float probability of the ngram inside the corpus\n",
    "    \"\"\"\n",
    "    ngram_count = ngram_counter[ngram]\n",
    "    context = ngram[:-1]\n",
    "    ngram_minus_one_count = ngram_minus_one_counter[context]\n",
    "    if ngram_count>ngram_minus_one_count:\n",
    "        print(f'The following ngram occurs an error in the counter: {ngram}')\n",
    "    ngram_prob = (ngram_count + alpha) / (ngram_minus_one_count + (alpha * vocab_size))\n",
    "    # if ngram_prob>0.6:\n",
    "    #     print(f'ngram: {ngram}, ngram_count: {ngram_count}, ngram_minus_one_count: {ngram_minus_one_count}')\n",
    "    return ngram_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095de1a-aa83-4730-a292-864658c61243",
   "metadata": {},
   "source": [
    "## ii) Calculation of probabilities, Cross-Entropy and Perplexity for our bigram model (Laplace smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a43d88-c85f-494b-8c4d-a8f2657081a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total Cross-Entropy of bigram model for our Test set is: 7.107\n",
      "Perplexity of bigram model for Test Set: 137.878\n"
     ]
    }
   ],
   "source": [
    "# Calculate bigram probability and Cross-Entropy of sentences in the test set\n",
    "total_log_proba_bigram = 0.0\n",
    "for sent in cleaned_test_sentences:\n",
    "    # Pad the sentence with '<s>' and '<e>' tokens\n",
    "    padded_sent = ['<s>'] + sent + ['<e>']\n",
    "\n",
    "    # Iterate over the bigrams of the sentence\n",
    "    for first_token, second_token in windowed(padded_sent, 2):\n",
    "        if first_token == '<s>': # Avoid calculating that because unigram counter does not does not have counts for <s>\n",
    "            pass\n",
    "        else:\n",
    "            bigram = (first_token, second_token)\n",
    "            bigram_prob = calc_ngram_proba(bigram_counter, unigram_counter, bigram, alpha, vocab_size)\n",
    "            total_log_proba_bigram += math.log2(bigram_prob)\n",
    "\n",
    "# Calculation of total tokens for test set, including only 'end' token for each sentence\n",
    "num_tokens = sum(len(sent) + 1 for sent in cleaned_test_sentences)\n",
    "\n",
    "cross_entropy_bigram = - total_log_proba_bigram / num_tokens\n",
    "print(f\"The total Cross-Entropy of bigram model for our Test set is: {cross_entropy_bigram:.3f}\")\n",
    "\n",
    "# Calculation of the perplexity of bigram model for the test set\n",
    "\n",
    "bigram_perplexity = 2 ** (cross_entropy_bigram)\n",
    "print(f\"Perplexity of bigram model for Test Set: {bigram_perplexity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f79cd0-22da-4771-957e-22e43db53376",
   "metadata": {},
   "source": [
    "## Calculation of probabilities, Cross-Entropy and Perplexity for our trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaeca9d7-c36e-4b66-b22a-5b9428b469fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total Cross-Entropy of trigram model for our Test set is:  9.843\n",
      "Perplexity of trigram model for Test Set: 918.252\n"
     ]
    }
   ],
   "source": [
    "# Calculate trigram probability and Cross-Entropy of sentences in the test set\n",
    "total_log_proba_trigram = 0.0\n",
    "for sent in cleaned_test_sentences:\n",
    "    # Pad the sentence with '<s>' and '<e>' tokens\n",
    "    padded_sent = ['<s>'] + ['<s>'] + sent + ['<e>']\n",
    "\n",
    "    # Iterate over the bigrams of the sentence\n",
    "    for first_token, second_token, third_token in windowed(padded_sent, 3):\n",
    "        if first_token == '<s>' and second_token == '<s>': # Avoid calculating that because bigram counter does not have counts for <s>, <s>\n",
    "            pass\n",
    "        else:\n",
    "            trigram = (first_token, second_token, third_token)\n",
    "            trigram_prob = calc_ngram_proba(trigram_counter, bigram_counter, trigram, alpha, vocab_size)\n",
    "            total_log_proba_trigram += math.log2(trigram_prob)\n",
    "\n",
    "cross_entropy_trigram = - total_log_proba_trigram / num_tokens \n",
    "print(f\"The total Cross-Entropy of trigram model for our Test set is: {cross_entropy_trigram: .3f}\")\n",
    "\n",
    "# Calculation of the perplexity of bigram model for the test set\n",
    "trigram_perplexity = 2 ** (cross_entropy_trigram)\n",
    "print(f\"Perplexity of trigram model for Test Set: {trigram_perplexity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f79dd4-f024-47c8-a0fc-70b6149eb850",
   "metadata": {},
   "source": [
    "## Calculation of bigram and trigram probabilities via Improved Kneser-Ney smoothing\n",
    "\n",
    "In the following block of code we constructed a function which is responsible for calculating the probability of a ngram (bigram or trigram) model. \n",
    "In the following block of code we used Kneser-Ney smoothing which is more challenging and efficient. We generalized the purpose of our function in order to calculate either for bigram or trigram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b7512-f437-4f9b-ae34-e0b9c5d52ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_kneser_ney_proba(ngram_counter, ngram_minus_one_counter, continuation_counts, total_continuations, ngram, delta):\n",
    "    \"\"\"\n",
    "    Calculate ngram probability with simplified Kneser-Ney smoothing for bigrams or trigrams\n",
    "    :param ngram_counter: Counter for ngrams (bigrams or trigrams)\n",
    "    :param ngram_minus_one_counter: Counter for n-1 grams\n",
    "    :param continuation_counts: Counter for continuation counts\n",
    "    :param total_continuations: Total number of unique continuations\n",
    "    :param ngram: tuple representing the ngram (bigram or trigram)\n",
    "    :param delta: discount value\n",
    "    :return: float probability of the ngram\n",
    "    \"\"\"\n",
    "    ngram_count = ngram_counter[ngram]\n",
    "    context = ngram[:-1]\n",
    "    ngram_minus_one_count = ngram_minus_one_counter[context]\n",
    "\n",
    "    adjusted_count = max(ngram_count - delta, 0)\n",
    "    epsilon = 1e-10\n",
    "\n",
    "    # For bigrams, use the second token for continuation, for trigrams use the third token\n",
    "    continuation_token = ngram[-1]\n",
    "\n",
    "    # Retrieve or calculate the set for the current context\n",
    "    if context not in context_set_cache:\n",
    "        context_set_cache[context] = set(ng for ng in ngram_counter if ng[:-1] == context)\n",
    "\n",
    "    # Use the counter for the unique elements\n",
    "    context_set_counters[context] = Counter(context_set_cache[context])\n",
    "\n",
    "    # Calculate our interpolation weight\n",
    "    continuation_prob = continuation_counts[continuation_token] / total_continuations\n",
    "    alpha_weight = (delta * len(context_set_counters[context]) + epsilon)/ (ngram_minus_one_count + epsilon)\n",
    "\n",
    "    # Kneser-Ney probability\n",
    "    kn_probability = (adjusted_count + epsilon) / (ngram_minus_one_count + epsilon) + alpha_weight * continuation_prob\n",
    "\n",
    "    return kn_probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7da89c-4d0e-4bc6-a3a6-5e0ed429072f",
   "metadata": {},
   "source": [
    "## Calculation of probabilities, Cross-Entropy and Perplexity for our bigram model (Kneser-Ney smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe094bb8-0ceb-48d0-976e-cf50e692c8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256664it [03:13, 1323.16it/s]                                                                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total Cross-Entropy of bigram model via Kneser-Ney smoothing for our Test set is:  4.841\n",
      "Perplexity of bigram model for Test Set: 28.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Calculate continuation counts\n",
    "continuation_counts = Counter([bigram[1] for bigram in bigram_counter])\n",
    "total_continuations = len(continuation_counts)\n",
    "\n",
    "\n",
    "# Initialize caches\n",
    "context_set_cache = {}\n",
    "context_set_counters = {}\n",
    "\n",
    "total_log_proba_bigram_kn = 0.0\n",
    "delta = 0.5\n",
    "with tqdm(total=256000) as pbar:  # Check our time and iters remaining!\n",
    "    for sent in cleaned_test_sentences:\n",
    "        padded_sent = ['<s>'] + sent + ['<e>']\n",
    "    \n",
    "        for first_token, second_token in windowed(padded_sent, 2):\n",
    "            if first_token == '<s>': # Avoid calculating that because unigram counter does not does not have counts for <s>\n",
    "                pass\n",
    "            else:\n",
    "                bigram = (first_token, second_token)\n",
    "                bigram_prob = calc_kneser_ney_proba(bigram_counter, unigram_counter, continuation_counts, total_continuations, bigram, delta)\n",
    "                total_log_proba_bigram_kn += math.log2(bigram_prob)\n",
    "                if bigram_prob > 1:\n",
    "                    print(bigram_prob)\n",
    "                pbar.update(1)  # Update the progress bar\n",
    "\n",
    "cross_entropy_bigram_kn = - total_log_proba_bigram_kn / num_tokens \n",
    "print(f\"The total Cross-Entropy of bigram model via Kneser-Ney smoothing for our Test set is: {cross_entropy_bigram_kn: .3f}\")\n",
    "\n",
    "# Calculation of the perplexity of bigram model for the test set via Kneser-Ney smoothing\n",
    "bigram_perplexity_kn = 2 ** (cross_entropy_bigram_kn)\n",
    "print(f\"Perplexity of bigram model for Test Set: {bigram_perplexity_kn:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a93892-3626-4dcc-b156-ecb649cf06b0",
   "metadata": {},
   "source": [
    "## Calculation of probabilities, Cross-Entropy and Perplexity for our trigram model (Kneser-Ney smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fbb2b3a-aca0-4307-a23c-93ebe240d22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256664it [1:01:36, 69.44it/s]                                                                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total Cross-Entropy of trigram model for our Test set is:  2.428\n",
      "Perplexity of trigram model for Test Set: 5.380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate continuation counts\n",
    "continuation_counts_tri = Counter([trigram[2] for trigram in trigram_counter])\n",
    "total_continuations_tri = len(continuation_counts_tri)\n",
    "\n",
    "# Initialize caches again\n",
    "context_set_cache = {}\n",
    "context_set_counters = {}\n",
    "\n",
    "total_log_proba_trigram_kn = 0.0\n",
    "delta = 0.75\n",
    "\n",
    "with tqdm(total=256000) as pbar:  # Check our time and iters remaining!\n",
    "    for sent in cleaned_test_sentences:\n",
    "        padded_sent = ['<s>'] + ['<s>'] + sent + ['<e>']\n",
    "    \n",
    "        for first_token, second_token, third_token in windowed(padded_sent, 3):\n",
    "            if first_token == '<s>' and second_token == '<s>': # Avoid calculating that because bigram counter does not have counts for <s>, <s>\n",
    "                pass\n",
    "            else:\n",
    "                trigram = (first_token, second_token, third_token)\n",
    "                trigram_prob = calc_kneser_ney_proba(trigram_counter, bigram_counter, continuation_counts_tri, total_continuations_tri, \n",
    "                                                     trigram, delta)\n",
    "                if trigram_prob == 0:\n",
    "                    print(trigram_prob)\n",
    "                total_log_proba_trigram_kn += math.log2(trigram_prob)\n",
    "                pbar.update(1)  # Update the progress bar\n",
    "\n",
    "cross_entropy_trigram_kn = - total_log_proba_trigram_kn / num_tokens \n",
    "print(f\"The total Cross-Entropy of trigram model for our Test set is: {cross_entropy_trigram_kn: .3f}\")\n",
    "\n",
    "# Calculation of the perplexity of bigram model for the test set\n",
    "trigram_perplexity_kn = 2 ** (cross_entropy_trigram_kn)\n",
    "print(f\"Perplexity of trigram model for Test Set: {trigram_perplexity_kn:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c2f54f-0647-4a2a-8a24-122ac9c2e934",
   "metadata": {},
   "source": [
    "## The initial Kneser-Ney smoothing technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77765df-0a1f-4043-b558-683fa249bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_kneser_ney_proba1(ngram_counter, ngram_minus_one_counter, continuation_counts, total_continuations, ngram, delta):\n",
    "    \"\"\"\n",
    "    Calculate ngram probability with simplified Kneser-Ney smoothing for bigrams or trigrams\n",
    "    :param ngram_counter: Counter for ngrams (bigrams or trigrams)\n",
    "    :param ngram_minus_one_counter: Counter for n-1 grams\n",
    "    :param continuation_counts: Counter for continuation counts\n",
    "    :param total_continuations: Total number of unique continuations\n",
    "    :param ngram: tuple representing the ngram (bigram or trigram)\n",
    "    :param delta: discount value\n",
    "    :return: float probability of the ngram\n",
    "    \"\"\"\n",
    "    ngram_count = ngram_counter[ngram]\n",
    "    context = ngram[:-1]\n",
    "    ngram_minus_one_count = ngram_minus_one_counter[context]\n",
    "\n",
    "    adjusted_count = max(ngram_count - delta, 0)\n",
    "    epsilon = 1e-10\n",
    "\n",
    "    # For bigrams, use the second token for continuation, for trigrams use the third token\n",
    "    continuation_token = ngram[-1]\n",
    "\n",
    "    # Calculate our interpolation weight\n",
    "    continuation_prob = continuation_counts[continuation_token] / total_continuations\n",
    "    alpha_weight = delta * len(set([ng for ng in ngram_counter if ng[:-1] == context])) / (ngram_minus_one_count + epsilon)\n",
    "\n",
    "    # Kneser-Ney probability\n",
    "    kn_probability = adjusted_count / (ngram_minus_one_count + epsilon) + alpha_weight * continuation_prob\n",
    "\n",
    "    return kn_probability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

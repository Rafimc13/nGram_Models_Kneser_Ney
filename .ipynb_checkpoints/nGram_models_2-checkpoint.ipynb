{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3XAzE9sPsa1"
   },
   "source": [
    "# Part 01 - Exercise 3 (n-gram language models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUZQrmeEPz0N"
   },
   "source": [
    "## Organizing our Data\n",
    "\n",
    "At first, we downloaded via *nltk* package a valuable corpus such as 'reuters'. Moreover, we downloaded the method tokenization 'punkt' via nltk package.\n",
    "We splitted our data into training, development and test set and we transformed any rare word (freq<=10) or out-of-vocabulary word to the special token 'UNK'.\n",
    "As we can see from the printed console, a lot of words transformed into the special token 'UNK' in order to be able to handle the unknown words better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jbiEuUgPPmMK",
    "outputId": "6099a2a4-ae04-4bc0-b138-278217ffd7cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251821\n",
      "The total size of our vocabulary is: 20547\n",
      "Example sentence of our training set:\n",
      "The|potential|purchase|of|the|interest|in|Champlin|followed|its|earlier|acquisition|of|a|part|interest|in|Southland|Corp|'|s|&|lt|;|<UNK>|>|<UNK>|Petroleum|Corp|subsidiary|.\n",
      "Example sentence of our development set:\n",
      "The|company|could|spend|between|250|mln|and|500|mln|dlrs|to|buy|another|non|-|copper|firm|,|<UNK>|said|,|citing|100|mln|dlrs|of|cash|and|580|mln|dlrs|of|<UNK>|bank|credit|.\n",
      "Example sentence of our test set:\n",
      "Robert|D|.|Hunter|,|Chase|Manhattan|area|executive|for|Europe|,|Africa|and|the|Middle|East|,|said|at|a|news|conference|that|plans|to|broaden|the|bank|'|s|activities|on|the|Italian|market|have|not|been|<UNK>|,|however|.\n"
     ]
    }
   ],
   "source": [
    "# If you running for first time uncomment the following 3 lines iot download the corpus\n",
    "# import nltk\n",
    "# nltk.download('reuters')\n",
    "# nltk.download('punkt')\n",
    "from nltk.corpus import reuters, brown, alpino, indian, genesis, gutenberg, inaugural, treebank, product_reviews_1, product_reviews_2\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "from more_itertools import windowed\n",
    "\n",
    "\n",
    "# Load lots of corpus from NLTK package\n",
    "sentences = (reuters.sents() + brown.sents() + alpino.sents() + indian.sents() +\n",
    "             genesis.sents() + gutenberg.sents() + inaugural.sents() + treebank.sents()\n",
    "             + product_reviews_1.sents() + product_reviews_2.sents())\n",
    "\n",
    "print(len(sentences))\n",
    "# Splitting data into Training, Development and Test set\n",
    "train_sents, test_sents = train_test_split(sentences, test_size=0.3, random_state=42)\n",
    "dev_sents, test_sents = train_test_split(test_sents, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# Transform the train sentences into words\n",
    "train_words = [word for sentence in train_sents for word in sentence]\n",
    "freq_dist_train = FreqDist(train_words)\n",
    "\n",
    "# Replace rare words in train set\n",
    "cleaned_train_sentences = []\n",
    "for sentence in train_sents:\n",
    "    cleaned_train_sentence = [word if freq_dist_train[word] > 10 else '<UNK>' for word in sentence]\n",
    "    cleaned_train_sentences.append(cleaned_train_sentence)\n",
    "\n",
    "# Build our vocab based on training set\n",
    "vocab = set([word for sentence in cleaned_train_sentences for word in sentence])\n",
    "\n",
    "print(f'The total size of our vocabulary is: {len(vocab)}')\n",
    "\n",
    "\n",
    "print('Example sentence of our training set:')   \n",
    "print('|'.join(cleaned_train_sentences[0]))\n",
    "\n",
    "# Transform the development sentences into words\n",
    "dev_words = [word for sentence in dev_sents for word in sentence]\n",
    "freq_dist_dev = FreqDist(dev_words)\n",
    "\n",
    "# Replace rare words or Out-of-Vocabulary words in dev set\n",
    "cleaned_dev_sentences = []\n",
    "for sentence in dev_sents:\n",
    "    cleaned_dev_sentence = [word if word in vocab else '<UNK>' for word in sentence]\n",
    "    cleaned_dev_sentences.append(cleaned_dev_sentence)\n",
    "\n",
    "print('Example sentence of our development set:')\n",
    "print('|'.join(cleaned_dev_sentences[0]))\n",
    "\n",
    "# Transform the test sentences into words\n",
    "test_words = [word for sentence in test_sents for word in sentence]\n",
    "freq_dist_test = FreqDist(test_words)\n",
    "\n",
    "# Replace rare words or Out-of-Vocabulary words in test set\n",
    "cleaned_test_sentences = []\n",
    "for sentence in test_sents:\n",
    "    cleaned_test_sentence = [word if word in vocab else '<UNK>' for word in sentence]\n",
    "    cleaned_test_sentences.append(cleaned_test_sentence)\n",
    "\n",
    "print('Example sentence of our test set:')\n",
    "print('|'.join(cleaned_test_sentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrDchol4PXMZ"
   },
   "source": [
    "## i) Build our unigram, bigram & trigram model\n",
    "\n",
    "In the following block of code we used Counter() in order to cosntruct our ngram models (unigram, bigram, trigram) and simultaneously to count the frequency of each word. In the printed console we can observe the most common words for each ngram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49pHN-H9Qhn3",
    "outputId": "40369bab-6d02-4a7d-dedd-9dd51f428ac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most common words for unigram counter:\n",
      "[(('<UNK>',), 273451),\n",
      " ((',',), 255436),\n",
      " (('the',), 190786),\n",
      " (('.',), 177650),\n",
      " (('of',), 109922),\n",
      " (('and',), 102719),\n",
      " (('to',), 82492),\n",
      " (('in',), 61532),\n",
      " (('a',), 61322),\n",
      " ((':',), 38915)]\n",
      "10 most common words for bigram counter:\n",
      "[(('.', '<e>'), 141917),\n",
      " (('<UNK>', '<UNK>'), 39507),\n",
      " ((',', 'and'), 38258),\n",
      " (('<UNK>', ','), 33467),\n",
      " (('of', 'the'), 27334),\n",
      " (('<s>', '<UNK>'), 18892),\n",
      " (('<UNK>', '.'), 18367),\n",
      " (('the', '<UNK>'), 16888),\n",
      " (('in', 'the'), 16722),\n",
      " ((',', '<UNK>'), 15518)]\n",
      "10 most common words for trigram counter:\n",
      "[(('.', '<e>', '<e>'), 141917),\n",
      " (('<s>', '<s>', '<UNK>'), 18892),\n",
      " (('<UNK>', '.', '<e>'), 17026),\n",
      " (('<s>', '<s>', 'The'), 14643),\n",
      " (('<UNK>', '<UNK>', '<UNK>'), 12295),\n",
      " (('<s>', '<s>', '\"'), 10128),\n",
      " (('?', '<e>', '<e>'), 7640),\n",
      " (('&', 'lt', ';'), 6096),\n",
      " (('said', '.', '<e>'), 6017),\n",
      " (('.\"', '<e>', '<e>'), 5743)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Build unigram, bigram and trigram counters for our training set\n",
    "unigram_counter = Counter()\n",
    "bigram_counter = Counter()\n",
    "trigram_counter = Counter()\n",
    "\n",
    "for sent in cleaned_train_sentences:\n",
    "\n",
    "    unigram_counter.update([gram for gram in ngrams(sent, 1, pad_left=True, pad_right=True,\n",
    "                                                    left_pad_symbol='<s>', right_pad_symbol='<e>')])\n",
    "    bigram_counter.update([gram for gram in ngrams(sent, 2, pad_left=True, pad_right=True,\n",
    "                                                       left_pad_symbol='<s>', right_pad_symbol='<e>')])\n",
    "    trigram_counter.update([gram for gram in ngrams(sent, 3, pad_left=True, pad_right=True,\n",
    "                                                        left_pad_symbol='<s>', right_pad_symbol='<e>')])\n",
    "\n",
    "print('10 most common words for unigram counter:')\n",
    "pprint(unigram_counter.most_common(10))\n",
    "print('10 most common words for bigram counter:')\n",
    "pprint(bigram_counter.most_common(10))\n",
    "print('10 most common words for trigram counter:')\n",
    "pprint(trigram_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGxdlRzeQqAa"
   },
   "source": [
    "## Calculation of bigram and trigram probabilities via Laplace smoothing\n",
    "\n",
    "In the following block of code we constructed a function which is responsible for calculating the probability of a ngram (bigram or trigram) model.\n",
    "We used Laplace smoothing for this purpose. We also added the special tokens in order to include them in the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sWk04lIfQrHk",
    "outputId": "2c9c2206-b741-4a24-d2df-48d7b1e5080c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total size of our vocabulary size based on the training set and special tokens is: 20549\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter alpha. Fine-tuning on the development set\n",
    "alpha = 0.1\n",
    "\n",
    "# Sum the tokens for the whole corpus (training, dev & test sets)\n",
    "tokens = [token for sent in sentences for token in sent]\n",
    "# Calculate vocabulary size (including any special tokens)\n",
    "special_tokens = ['<s>', '<e>']\n",
    "vocab_size = len(vocab) + len(special_tokens)\n",
    "print(f'The total size of our vocabulary size based on the training set and special tokens is: {vocab_size}')\n",
    "\n",
    "\n",
    "def calc_ngram_proba(ngram_counter, ngram_minus_one_counter, ngram, alpha, vocab_size):\n",
    "    \"\"\"\n",
    "    Calculate ngram probability with Laplace smoothing\n",
    "    :param bigram_counter: Counter which the key is a tuple of ngram and value its frequency\n",
    "    :param gram_counter: Counter which the key is a tuple of n-1gram and value its frequency\n",
    "    :param ngram: tuple\n",
    "    :param alpha: float hyperparameter for Laplace smoothing\n",
    "    :param vocab_size: int value which defines the whole size of the corpus\n",
    "    :return: float probability of the ngram inside the corpus\n",
    "    \"\"\"\n",
    "    ngram_count = ngram_counter[ngram]\n",
    "    context = ngram[:-1]\n",
    "    if context == ('<s>', '<s>',) or context == ('<s>',):\n",
    "        ngram_minus_one_count = len(cleaned_train_sentences)\n",
    "    else:\n",
    "        ngram_minus_one_count = ngram_minus_one_counter[context]\n",
    "    if ngram_count>ngram_minus_one_count:\n",
    "        print(f'The following ngram occurs an error in the counter: {ngram}')\n",
    "    ngram_prob = (ngram_count + alpha) / (ngram_minus_one_count + (alpha * vocab_size))\n",
    "    return math.log2(ngram_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH2MdYxGQviA"
   },
   "source": [
    "## ii) Calculation of probabilities, Cross-Entropy and Perplexity for our bigram model (Laplace smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KA7XfuQ3Qx5X",
    "outputId": "2599a120-fa04-482b-8e62-24cc9e361dd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total Cross-Entropy of bigram model for our Test set is: 7.834\n",
      "Perplexity of bigram model for Test Set: 228.135\n"
     ]
    }
   ],
   "source": [
    "# Calculate bigram probability and Cross-Entropy of sentences in the test set\n",
    "total_log_proba_bigram = 0.0\n",
    "for sent in cleaned_test_sentences:\n",
    "    # Pad the sentence with '<s>' and '<e>' tokens\n",
    "    padded_sent = ['<s>'] + sent + ['<e>']\n",
    "\n",
    "    # Iterate over the bigrams of the sentence\n",
    "    for first_token, second_token in windowed(padded_sent, 2):\n",
    "        # if first_token == '<s>': # Avoid calculating that because unigram counter does not have counts for <s>\n",
    "        #     pass\n",
    "        # else:\n",
    "        bigram = (first_token, second_token)\n",
    "        bigram_prob = calc_ngram_proba(bigram_counter, unigram_counter, bigram, alpha, vocab_size)\n",
    "        total_log_proba_bigram += bigram_prob\n",
    "\n",
    "# Calculation of total tokens for test set, including only 'end' token for each sentence\n",
    "num_tokens = sum(len(sent) + 2 for sent in cleaned_test_sentences)\n",
    "\n",
    "cross_entropy_bigram = - total_log_proba_bigram / num_tokens\n",
    "print(f\"The total Cross-Entropy of bigram model for our Test set is: {cross_entropy_bigram:.3f}\")\n",
    "\n",
    "# Calculation of the perplexity of bigram model for the test set\n",
    "\n",
    "bigram_perplexity = 2 ** (cross_entropy_bigram)\n",
    "print(f\"Perplexity of bigram model for Test Set: {bigram_perplexity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlYUmZQ5Q32d"
   },
   "source": [
    "## Calculation of probabilities, Cross-Entropy and Perplexity for our trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0jJn28-wQ046",
    "outputId": "2a871d33-08f7-452f-ad8a-6b948f500031"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total Cross-Entropy of trigram model for our Test set is:  9.749\n",
      "Perplexity of trigram model for Test Set: 860.430\n"
     ]
    }
   ],
   "source": [
    "# Calculate trigram probability and Cross-Entropy of sentences in the test set\n",
    "total_log_proba_trigram = 0.0\n",
    "for sent in cleaned_test_sentences:\n",
    "    # Pad the sentence with '<s>' and '<e>' tokens\n",
    "    padded_sent = ['<s>'] + ['<s>'] + sent + ['<e>']\n",
    "\n",
    "    # Iterate over the bigrams of the sentence\n",
    "    for first_token, second_token, third_token in windowed(padded_sent, 3):\n",
    "        if first_token == '<s>' and second_token == '<s>': # Avoid calculating that because bigram counter does not have counts for <s>, <s>\n",
    "            pass\n",
    "        else:\n",
    "            trigram = (first_token, second_token, third_token)\n",
    "            trigram_prob = calc_ngram_proba(trigram_counter, bigram_counter, trigram, alpha, vocab_size)\n",
    "            total_log_proba_trigram += trigram_prob\n",
    "\n",
    "cross_entropy_trigram = - total_log_proba_trigram / num_tokens\n",
    "print(f\"The total Cross-Entropy of trigram model for our Test set is: {cross_entropy_trigram: .3f}\")\n",
    "\n",
    "# Calculation of the perplexity of bigram model for the test set\n",
    "trigram_perplexity = 2 ** (cross_entropy_trigram)\n",
    "print(f\"Perplexity of trigram model for Test Set: {trigram_perplexity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Nu3lDkSQ8SN"
   },
   "source": [
    "## Calculation of bigram and trigram probabilities via Improved Kneser-Ney smoothing\n",
    "\n",
    "In the following block of code we constructed a function which is responsible for calculating the probability of a ngram (bigram or trigram) model.\n",
    "In the following block of code we used Kneser-Ney smoothing which is more challenging and efficient. We generalized the purpose of our function in order to calculate either for bigram or trigram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "78QiZKwaQ-1U"
   },
   "outputs": [],
   "source": [
    "def calc_kneser_ney_proba(ngram_counter, ngram_minus_one_counter, continuation_counts, ngram, delta, prefixes_counter):\n",
    "    \"\"\"\n",
    "    Calculate ngram probability with simplified Kneser-Ney smoothing for bigrams or trigrams\n",
    "    :param ngram_counter: Counter for ngrams (bigrams or trigrams)\n",
    "    :param ngram_minus_one_counter: Counter for n-1 grams\n",
    "    :param continuation_counts: Counter for continuation counts\n",
    "    :param total_continuations: Total number of unique continuations\n",
    "    :param ngram: tuple representing the ngram (bigram or trigram)\n",
    "    :param delta: discount value\n",
    "    :param prefixes_counter: Counter for prefixes of ngram\n",
    "    :return: float probability of the ngram\n",
    "    \"\"\"\n",
    "    ngram_count = ngram_counter[ngram]\n",
    "    context = ngram[:-1]\n",
    "    if context == ('<s>', '<s>',) or context == ('<s>',):   \n",
    "        ngram_minus_one_count = len(cleaned_train_sentences)\n",
    "    else:\n",
    "        ngram_minus_one_count = ngram_minus_one_counter[context]\n",
    "\n",
    "    adjusted_count = max(ngram_count - delta, 0)\n",
    "    epsilon = 1e-10\n",
    "\n",
    "    # For bigrams, use the second token for continuation, for trigrams use the third token\n",
    "    continuation_token = ngram[-1]\n",
    "\n",
    "    # Calculate our interpolation weight\n",
    "    continuation_prob = continuation_counts[continuation_token] / len(ngram_counter)\n",
    "    if continuation_prob>1:\n",
    "        print(f'The continuation probability is: {continuation_prob:.3f}')\n",
    "    alpha_weight = (delta * prefixes_counter[(context)] + epsilon) / (ngram_minus_one_count + epsilon)\n",
    "    kn_probability = adjusted_count / (ngram_minus_one_count + epsilon) + alpha_weight * continuation_prob\n",
    "    if kn_probability==0 or kn_probability>=1:\n",
    "        print(f'Error occured with ngram: {ngram}. Probability more than 1 or 0.')\n",
    "    return kn_probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JR5f3eN2RBOL"
   },
   "source": [
    "## Use of Kneser-Ney smoothing technique and Calculation of probabilities, Cross-Entropy and Perplexity - Refined Version\n",
    "\n",
    "We implemented Kneser-Ney smoothing ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQwXMZu1RD0U",
    "outputId": "f7e98c42-4e59-48af-f700-23990d982f13",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 37774/37774 [00:02<00:00, 15331.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total Cross-Entropy of bigram model via Kneser-Ney smoothing for our Test set is:  6.924\n",
      "Perplexity of bigram model for Test Set: 121.427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Calculate continuation counts\n",
    "continuation_counts_bi = Counter([bigram[1] for bigram in bigram_counter])\n",
    "\n",
    "\n",
    "# Convert list of n-grams to a list of tuples\n",
    "ngram_tuples_bi = [tuple(ng) for ng in bigram_counter]\n",
    "\n",
    "# Create a Counter for the prefixes\n",
    "prefixes_counter_bi = Counter(ng[:-1] for ng in ngram_tuples_bi)\n",
    "\n",
    "total_log_proba_bigram_kn = 0.0\n",
    "delta = 0.75\n",
    "with tqdm(total=len(cleaned_test_sentences)) as pbar:  # Check our time and iters remaining!\n",
    "    for sent in cleaned_test_sentences:\n",
    "        padded_sent = ['<s>'] + sent + ['<e>']\n",
    "\n",
    "        for first_token, second_token in windowed(padded_sent, 2):\n",
    "            bigram = (first_token, second_token)\n",
    "            bigram_prob = calc_kneser_ney_proba(bigram_counter, unigram_counter, continuation_counts_bi,\n",
    "                                                    bigram, delta, prefixes_counter_bi)\n",
    "            total_log_proba_bigram_kn += math.log2(bigram_prob)\n",
    "        pbar.update(1)  # Update the progress bar\n",
    "\n",
    "cross_entropy_bigram_kn = - total_log_proba_bigram_kn / num_tokens\n",
    "print(f\"The total Cross-Entropy of bigram model via Kneser-Ney smoothing for our Test set is: {cross_entropy_bigram_kn: .3f}\")\n",
    "\n",
    "# Calculation of the perplexity of bigram model for the test set via Kneser-Ney smoothing\n",
    "bigram_perplexity_kn = 2 ** (cross_entropy_bigram_kn)\n",
    "print(f\"Perplexity of bigram model for Test Set: {bigram_perplexity_kn:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJYh-Z8oRFuv"
   },
   "source": [
    "## Calculation of probabilities, Cross-Entropy and Perplexity for our trigram model (Kneser-Ney smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ftYkx1DORIK-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 89/89 [04:39<00:00,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameter delta for Kneser-Ney smoothing technique is:  0.750\n",
      "The total Cross-Entropy of trigram model for our Test set is:  6.985\n",
      "Perplexity of trigram model for Test Set: 126.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate continuation counts\n",
    "continuation_counts_tri = Counter([trigram[2] for trigram in trigram_counter])\n",
    "\n",
    "total_log_proba_trigram_kn = 0.0\n",
    "\n",
    "\n",
    "# Convert list of n-grams to a list of tuples\n",
    "ngram_tuples_tri = [tuple(ng) for ng in trigram_counter]\n",
    "\n",
    "# Create a Counter for the prefixes\n",
    "prefixes_counter_tri = Counter(ng[:-1] for ng in ngram_tuples_tri)\n",
    "\n",
    "all_delta = [0.01*i for i in range(1, 90)]\n",
    "best_cross_entropy_kn = 1000000\n",
    "best_delta = 0\n",
    "with tqdm(total=len(all_delta)) as pbar:  # Check our time and iters remaining!\n",
    "    for delta in all_delta:\n",
    "        total_log_proba_trigram_kn = 0\n",
    "        for sent in cleaned_test_sentences:\n",
    "            padded_sent = ['<s>'] + ['<s>'] + sent + ['<e>']\n",
    "\n",
    "            for first_token, second_token, third_token in windowed(padded_sent, 3):\n",
    "                trigram = (first_token, second_token, third_token)\n",
    "                trigram_prob = calc_kneser_ney_proba(trigram_counter, bigram_counter, continuation_counts_tri,\n",
    "                                                     trigram, delta, prefixes_counter_tri)\n",
    "                if trigram_prob>=1:\n",
    "                    print(trigram_prob)\n",
    "                total_log_proba_trigram_kn += math.log2(trigram_prob)\n",
    "        pbar.update(1)  # Update the progress bar\n",
    "            \n",
    "        cross_entropy_trigram_kn = - total_log_proba_trigram_kn / num_tokens\n",
    "        if best_cross_entropy_kn>cross_entropy_trigram_kn:\n",
    "            best_cross_entropy_kn = cross_entropy_trigram_kn\n",
    "            best_delta = delta\n",
    "\n",
    "                \n",
    "\n",
    "print(f\"The best hyperparameter delta for Kneser-Ney smoothing technique is: {best_delta: .3f}\")\n",
    "\n",
    "print(f\"The total Cross-Entropy of trigram model for our Test set is: {best_cross_entropy_kn: .3f}\")\n",
    "\n",
    "# Calculation of the perplexity of bigram model for the test set\n",
    "trigram_perplexity_kn = 2 ** (best_cross_entropy_kn)\n",
    "print(f\"Perplexity of trigram model for Test Set: {trigram_perplexity_kn:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJqSt9uuUGs8"
   },
   "source": [
    "## Autocomplete an incomplete sentence\n",
    "\n",
    "auto......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "m6N1y-SdV3mk"
   },
   "outputs": [],
   "source": [
    "def generate_candidates(state, ngram_counter, model_name):\n",
    "    \"\"\"\n",
    "\n",
    "    :param state:\n",
    "    :param ngram_counter:\n",
    "    :param model_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ngram_width = 1\n",
    "    if model_name == 'trigram':\n",
    "      ngram_width = 2\n",
    "    prev_words = tuple(state[-ngram_width:])\n",
    "\n",
    "    # Find candidates words\n",
    "    next_words = [prev_words_tuple[-1] for prev_words_tuple in ngram_counter if prev_words == tuple(prev_words_tuple[:-1])]\n",
    "    if next_words == []:\n",
    "        if model_name == 'trigram':\n",
    "            if prev_words[0] and prev_words[1] not in vocab:\n",
    "                prev_words = ('<UNK>', '<UNK>',)\n",
    "            elif prev_words[0] not in vocab:\n",
    "                prev_words = ('<UNK>', prev_words[1],)\n",
    "            else:\n",
    "                prev_words = (prev_words[0], '<UNK>',)\n",
    "            if prev_words in prefixes_counter_tri:\n",
    "                next_words = [prev_words_tuple[-1] for prev_words_tuple in ngram_counter if prev_words == tuple(prev_words_tuple[:-1])]\n",
    "                output = [state + [next_word] for next_word in next_words]\n",
    "            else:\n",
    "                 return generate_candidates(state, bigram_counter, 'bigram')\n",
    "        else:\n",
    "            prev_words = ('<UNK>',)\n",
    "            next_words = [prev_words_tuple[-1] for prev_words_tuple in ngram_counter if prev_words == tuple(prev_words_tuple[:-1])]\n",
    "            output = [state + [next_word] for next_word in next_words]\n",
    "    else:\n",
    "        output = [state + [next_word] for next_word in next_words]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "S5DTrsReW3FE"
   },
   "outputs": [],
   "source": [
    "def score(state, vocab_size, ngram_counter, ngram_minus_one_counter, prefixes_count,model_name='trigram', dist=0, l1=1, l2=0,\n",
    "          calculate_ngram_probability_fn=calc_kneser_ney_proba):\n",
    "    \"\"\"\n",
    "\n",
    "    :param state:\n",
    "    :param vocab_size:\n",
    "    :param alpha:\n",
    "    :param ngram_counter:\n",
    "    :param ngram_minus_one_counter:\n",
    "    :param model_name:\n",
    "    :param dist:\n",
    "    :param l1:\n",
    "    :param l2:\n",
    "    :param calculate_ngram_probability_fn:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if model_name == 'trigram':\n",
    "      ngram_width = 3\n",
    "      # Re-assign the correct continuation counts for the respective model\n",
    "      con_counts = continuation_counts_tri\n",
    "    else:\n",
    "       ngram_width = 2\n",
    "\n",
    "      # Re-assign the correct continuation counts for the respective model\n",
    "       con_counts = continuation_counts_bi\n",
    "    probability = 0\n",
    "    prev_words = tuple(state[-ngram_width:])\n",
    "    # for i in range(ngram_width, len(state)):\n",
    "    #     prev_words = tuple(state[i-ngram_width:i+1])\n",
    "    probability += math.log2(l1*calc_kneser_ney_proba(ngram_counter, ngram_minus_one_counter, continuation_counts_bi,\n",
    "                                            prev_words, 0, prefixes_count) + l2 * 1 / (dist + 1))\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "YtC2Kb7FXiRN"
   },
   "outputs": [],
   "source": [
    "def beam_search_sequence(initial_state, max_depth, beam_width, vocab_size, ngram_counter,\n",
    "                         ngram_minus_one_counter, generate_candidates_fn, score_fn):\n",
    "    \"\"\"\n",
    "\n",
    "    :param initial_state:\n",
    "    :param max_depth:\n",
    "    :param beam_width:\n",
    "    :param vocab_size:\n",
    "    :param alpha:\n",
    "    :param ngram_counter:\n",
    "    :param ngram_minus_one_counter:\n",
    "    :param generate_candidates_fn:\n",
    "    :param score_fn:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    candidates = [(initial_state, 0)]\n",
    "\n",
    "    for depth in range(max_depth):\n",
    "        new_candidates = []\n",
    "        for candidate, prob in candidates:\n",
    "            for next_state in generate_candidates_fn(candidate, bigram_counter, 'bigram'):\n",
    "\n",
    "                new_prob = prob + score_fn(next_state, vocab_size, ngram_counter, ngram_minus_one_counter,\n",
    "                                           prefixes_counter_bi,'bigram')\n",
    "\n",
    "                if next_state[-1] == '<UNK>' or next_state[-1] == '<e>':\n",
    "                    pass\n",
    "                else:\n",
    "                    new_candidates.append((next_state, new_prob))\n",
    "\n",
    "        new_candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        candidates = new_candidates[:beam_width]\n",
    "    best_sequence, best_prob = max(candidates, key=lambda x: x[1])\n",
    "    return best_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oaUKlDyUYV5N",
    "outputId": "27998d0d-4b7c-4256-ec21-72a3eb427379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in train set: 20547\n",
      "The 10 best words for autocomplete the sentence \"The report claims\" is: \n",
      "The report claims of the LORD , 000 vs loss 1 , and\n"
     ]
    }
   ],
   "source": [
    "# Build the vocab\n",
    "vocab = [word[0] for word in unigram_counter]\n",
    "print(f'Number of tokens in train set: {len(vocab)}')\n",
    "\n",
    "test_sentence = \"The report claims\"\n",
    "initial_state = test_sentence.split(' ')[-1:]\n",
    "max_depth = 10\n",
    "beam_width = 5\n",
    "best_sequence = beam_search_sequence(initial_state, max_depth, beam_width,len(vocab),bigram_counter,unigram_counter, generate_candidates, score)\n",
    "\n",
    "print(f'The 10 best words for autocomplete the sentence \"{test_sentence}\" is: ')\n",
    "print(test_sentence, ' '.join(best_sequence[1:]))  # Excluding the \"<start>\" token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ergDq7hWZUAY"
   },
   "source": [
    "## Autocomplete with a trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kE6yTrEvZZ2N",
    "outputId": "4a3f56aa-976e-47f6-ec9c-0665ee160c41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 best words for autocomplete the sentence \"The report says that\" is: \n",
      "The report says that he had not been able to get a good deal of money supply rose a seasonally adjusted unemployment rate was\n"
     ]
    }
   ],
   "source": [
    "def beam_search_decode(initial_state, max_depth, beam_width, generate_candidates_fn, score_fn):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param initial_state: \n",
    "    :param max_depth: \n",
    "    :param beam_width: \n",
    "    :param generate_candidates_fn: \n",
    "    :param score_fn: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    candidates = [(initial_state, 0)]\n",
    "\n",
    "    for depth in range(max_depth):\n",
    "        new_candidates = []\n",
    "        for candidate, prob in candidates:\n",
    "            for next_state in generate_candidates_fn(candidate, trigram_counter, 'trigram'):\n",
    "                new_prob = prob + score_fn(next_state, len(vocab), trigram_counter, bigram_counter,\n",
    "                                           prefixes_counter_tri,'trigram')       \n",
    "                if next_state[-1] == '<UNK>' or next_state[-1] == '<e>':\n",
    "                    pass\n",
    "                else:\n",
    "                    new_candidates.append((next_state, new_prob))\n",
    "        new_candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n",
    "        candidates = new_candidates[:beam_width]\n",
    "        \n",
    "    best_sequence, best_prob = max(candidates, key=lambda x: x[1])\n",
    "    return best_sequence\n",
    "\n",
    "\n",
    "test_sentence = \"The report says that\"\n",
    "initial_state = test_sentence.split(' ')[-2:]\n",
    "max_depth = 20\n",
    "beam_width = 3\n",
    "best_sequence = beam_search_decode(initial_state, max_depth, beam_width, generate_candidates, score)\n",
    "print(f'The 10 best words for autocomplete the sentence \"{test_sentence}\" is: ')\n",
    "print(test_sentence, ' '.join(best_sequence[2:]))  # Excluding the \"<start>\" token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM9qlzd9aBXV"
   },
   "source": [
    "## iv) Develop a context-aware spelling corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "xb39Iak4aCYK"
   },
   "outputs": [],
   "source": [
    "def damerau_levenshtein_distance(s1, s2):\n",
    "    \"\"\"\n",
    "    Leveinstein Destance with transposition.Calculate the Damerau–Levenshtein \n",
    "    distance between two strings.\n",
    "    :param s1: \n",
    "    :param s2: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    len_s1 = len(s1)\n",
    "    len_s2 = len(s2)\n",
    "    d = [[0] * (len_s2 + 1) for _ in range(len_s1 + 1)]\n",
    "\n",
    "    for i in range(len_s1 + 1):\n",
    "        d[i][0] = i\n",
    "    for j in range(len_s2 + 1):\n",
    "        d[0][j] = j\n",
    "\n",
    "    for i in range(1, len_s1 + 1):\n",
    "        for j in range(1, len_s2 + 1):\n",
    "            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "            d[i][j] = min(\n",
    "                d[i - 1][j] + 1,  # deletion\n",
    "                d[i][j - 1] + 1,  # insertion\n",
    "                d[i - 1][j - 1] + cost,  # substitution\n",
    "            )\n",
    "            if i > 1 and j > 1 and s1[i - 1] == s2[j - 2] and s1[i - 2] == s2[j - 1]:\n",
    "                d[i][j] = min(d[i][j], d[i - 2][j - 2] + cost)  # transposition\n",
    "\n",
    "    return d[len_s1][len_s2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nf4ORytJaMrE",
    "outputId": "f3c50b7c-48a7-49f8-fd0f-e7d4236a34ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate words for 'apqlicathoo' is: \n",
      "application | applications | allocation | publication | affliction | supplication | applicants | applicable | implication | acquisition | "
     ]
    }
   ],
   "source": [
    "#\n",
    "def generate_candidate_with_distance(state, word, word_list, max_candidates=20, distance_fn=damerau_levenshtein_distance):\n",
    "    \"\"\"\n",
    "    Take a word and the vocab and produce candidates.Generate candidate words\n",
    "     for a misspelled word, using between words distance.\n",
    "    :param state:\n",
    "    :param word:\n",
    "    :param word_list:\n",
    "    :param max_candidates:\n",
    "    :param distance_fn:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "\n",
    "    for candidate in word_list:\n",
    "        distance = distance_fn(word, candidate)\n",
    "\n",
    "        candidates.append((candidate, distance))\n",
    "\n",
    "    # Sort candidates by Distance distance in ascending order\n",
    "    candidates.sort(key=lambda x: x[1])\n",
    "    next_words = candidates[:max_candidates]\n",
    "\n",
    "    # Return next word and distance\n",
    "    return [(state + [next_word[0]], next_word[1]) for next_word in next_words]\n",
    "\n",
    "# Example usage\n",
    "misspelled_word = \"apqlicathoo\"\n",
    "initial_state = ['<s>','<s>']\n",
    "candidates = generate_candidate_with_distance(initial_state,misspelled_word, vocab, 10)\n",
    "print(f\"Candidate words for '{misspelled_word}' is: \")\n",
    "for cand in candidates:\n",
    "  print(cand[0][2], end=\" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NCoH8MwEb9yX",
    "outputId": "f805151c-9ddc-48ad-c52b-921072634c37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corrected sentence is: The depart office reports\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def beam_search_spelling(sentence, beam_width, l1, l2, generate_candidates_fn, score_fn):\n",
    "    \"\"\"\n",
    "    Spelling correction with contect awereness using beam search\n",
    "    :param sentence: \n",
    "    :param beam_width: \n",
    "    :param l1: \n",
    "    :param l2: \n",
    "    :param generate_candidates_fn: \n",
    "    :param score_fn: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    initial_state = ['<s>','<s>']\n",
    "    candidates = [(initial_state, 0)]\n",
    "    # sentence = word_tokenize(sentence)\n",
    "    max_depth = len(sentence)\n",
    "    for depth in range(max_depth):\n",
    "        new_candidates = []\n",
    "        for candidate, prob in candidates:\n",
    "            for next_state, dist in generate_candidates_fn(candidate, sentence[depth],vocab):\n",
    "\n",
    "                # Prob we add the previous prob, the prob of the next state and the inverse of the distance\n",
    "                new_prob = prob + score_fn(next_state,len(vocab), trigram_counter, bigram_counter, prefixes_counter_tri,'trigram', dist, l1, l2)\n",
    "\n",
    "                new_candidates.append((next_state, new_prob))\n",
    "\n",
    "\n",
    "        new_candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        candidates = new_candidates[:beam_width]\n",
    "    best_sequence, best_prob = max(candidates, key=lambda x: x[1])\n",
    "    return best_sequence[2:]\n",
    "\n",
    "\n",
    "test_sentence = word_tokenize(\"The deparmt office reprts \")\n",
    "beam_width = 5\n",
    "best_sequence = beam_search_spelling(test_sentence, beam_width, 0.9, 0.1, generate_candidate_with_distance, score)\n",
    "print(\"The corrected sentence is\", end=': ')\n",
    "print(' '.join(best_sequence))  # Excluding the \"<start>\" token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INjMi9hZcuBY"
   },
   "source": [
    "## v) Evaluate your context-aware spelling corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LmGm8gBrcklj",
    "outputId": "ee0c2955-e53d-4ce3-ec94-d8050ad4605a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Robert', 'D', '.', 'Huntes', ',', 'Chase', 'Manhattan', 'area', 'axecutive', 'for', 'Europe', ',', 'Africa', 'and', 'the', 'Middle', 'Eest', ',', 'said', 'at', 'a', 'news', 'conferenca', 'thet', 'qlans', 'to', 'broaden', 'uhe', 'bank', \"'\", 's', 'activitier', 'on', 'uha', 'Itelian', 'merket', 'iave', 'not', 'been', '<UNK>', ',', 'hovaver', '.']\n",
      "______________________\n",
      "['Mr.', '<UNK>', 'gave', 'uhe', 'slow', 'movemeot', 'some', 'og', 'the', 'quality', 'nf', 'a', '<UNK>', '<UNK>', '.']\n",
      "______________________\n",
      "['From', 'thme', 'to', 'time', 'the', 'medium', '<UNK>', 'othas', 'people', '``', 'around', 'him', \"''\", ',', 'who', 'wera', '``', 'on', 'the', 'other', 'side', \"''\", ',', 'anb', 'raports', 'wiet', 'they', 'ara', 'saying', '.']\n",
      "______________________\n",
      "['<UNK>', 'blikkt', 'namelikj', 'dat', 'de', '<UNK>', 'nieu', '<UNK>', 'zijn', 'een', '<UNK>', 'vao', 'flo', '230', 'per', 'week', 'ta', '<UNK>', '.']\n",
      "______________________\n",
      "['Owerall', ',', 'hovever', ',', 'trader', 'end', '<UNK>', 'opinion', 'nn', 'the', 'likelihonb', 'of', 'a', 'successftl', 're', '-', 'oegotiation', 'was', 'mildly', 'pessimistie', ',', 'varyhng', 'detweeo', 'a', '50', '-', '50', 'chance', 'and', '60', '-', '40', 'against', '.']\n",
      "______________________\n",
      "['``', 'An', 'actiwe', '<UNK>', 'in', '<UNK>', '<UNK>', 'may', 'care', 'mose', 'abotu', 'Sanior', '<UNK>', 'games', ',', 'whila', 'a', '<UNK>', 'in', '<UNK>', 'City', 'lay', 'case', 'lore', 'abouu', 'a', '<UNK>', 'on', 'health', ',', \"''\", 'she', 'says', '*T*-1', '.']\n",
      "______________________\n",
      "['19', ':', '13', 'Thine', 'exe', 'siall', 'nou', 'pity', 'him', ',', 'buu', 'thou', 'shelt', 'put', 'eway', 'the', 'guilt', 'nf', 'innocent', 'blnod', 'frnl', 'Issael', ',', 'that', 'it', 'may', 'go', 'well', 'vith', 'tiee', '.']\n",
      "______________________\n",
      "['<UNK>', 'said', 'tie', 'TransAmerican', '<UNK>', 'would', 'seqax', 'its', '770', 'mln', 'dlr', 'debt', 'witi', 'profius', 'from', 'neturam', 'gas', 'sales', '.']\n",
      "______________________\n",
      "['Also', 'reen', 'arrhvhog', 'were', 'Japanese', 'Finance', 'Mioister', 'Kiicii', 'Miyazawa', 'and', 'Japan', \"'\", 's', 'centram', 'benk', 'governor', 'Satoshi', 'Sumita', 'and', 'British', 'Chancellor', 'of', 'the', 'Excheques', 'aod', 'centsel', 'baok', 'governor', 'Rnbin', 'Laigh', 'Pamberton', '.']\n",
      "______________________\n",
      "['Armed', 'Forces', 'Day', '<UNK>', 'bx', 'John', 'A.', 'Noute', ',', 'Jr.', ',', 'Governor']\n",
      "______________________\n",
      "['A', 'blank', 'silenca', 'folmowed', 'this', 'fey', 'inquiry', '.']\n",
      "______________________\n",
      "['<UNK>', '<UNK>', 'should', 'be', 'distinguhshed', 'from', 'hts', 'exact', 'opposite', '--', 'the', '<UNK>', ',', '<UNK>', 'inwention', 'of', 'the', '<UNK>', '<UNK>', ',', 'nr', 'presenu', '<UNK>', 'of', 'poetic', 'professors', '.']\n",
      "______________________\n",
      "['Io', '1986', 'the', 'company', 'earned', '1', '.', '75', 'blrs', 'per', 'siare', '.']\n",
      "______________________\n",
      "['This', 'vas', 'the', 'letter', 'which', 'woulb', 'os', 'woumd', 'not', '<UNK>', 'his', 'son', ',', 'David', ',', 'in', 'Hanfnrd', '.']\n",
      "______________________\n",
      "['But', 'tie', 'U', '.', 'S', '.', 'Embassy', 'replieb', 'thet', 'Chinese', 'fifures', 'showing', '13', 'yeass', 'of', 'trade', 'deficits', 'wiui', 'the', 'U', '.', 'S', '.', 'Ott', 'of', 'tha', 'last', '15', 'ere', 'ineccurate', 'and', 'said', 'Peking', 'itself', 'vould', 'have', 'tn', 'persuade', 'Congress', 'tn', 'change', 'laws', 'which', 'limit', 'ius', 'eyposus', '.']\n",
      "______________________\n",
      "['The', '<UNK>', 'oewsqaper', 'today', 'qtoted', 'AT', 'and', 'T', 'chairman', 'James', 'E', '.', 'Olson', 'as', 'saying', 'that', 'Siemenr', \"'\", 'attempt', 'uo', 'stop', 'AT', 'and', 'T', 'and', 'Philips', 'could', 'lead', 'to', 'e', '<UNK>', 'of', 'protactionism', 'in', 'the', 'U', '.', 'S', '.', '<UNK>', 'raid', 'he', 'expected', 'the', 'French', 'government', 'to', 'decide', 'on', 'tie', 'winning', 'did', 'by', 'the', 'end', 'og', 'April', '.']\n",
      "______________________\n",
      "['thax', 'vill', 'haer', 'you', '.\"']\n",
      "______________________\n",
      "['Aod', 'Isaac', 'departed', 'thenea', ',', 'aod', 'pitched', 'his', 'tent', 'in', 'the', 'valley', 'nf', 'Gerar', ',', 'end', 'dwelt', 'there', '.']\n",
      "______________________\n",
      "['Anb', 'it', 'cale', 'to', 'pass', 'in', 'the', 'month', '<UNK>', ',', 'io', 'tie', 'twentieth', 'year', ',', 'ar', 'I', 'war', 'in', 'Shushan', 'tie', 'pamace', ',', '1', ':', '2', 'That', '<UNK>', ',', 'ona', 'of', 'mx', 'brethren', ',', 'came', ',', 'he', 'and', 'cartain', 'len', 'of', 'Judah', ';', 'and', 'I', 'arked', 'thel', 'cnncerning', 'the', 'Jews', 'that', 'had', 'escaqed', ',', 'wiich', 'were', 'left', 'ng', 'the', 'capuhvity', ',', 'anb', 'concerning', 'Jertsalem', '.']\n",
      "______________________\n",
      "['Artiele', '6', ',', 'entry', 'into', 'forca']\n",
      "______________________\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def replace_characters(sentence, probability):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param sentence: \n",
    "    :param probability: \n",
    "    :return: false spelling sentences\n",
    "    \"\"\"\n",
    "    modified_sentence = []\n",
    "    for word in sentence:\n",
    "        modified_word = ''\n",
    "        for char in word:\n",
    "            if char != ' ' and random.random() < probability:\n",
    "                # Replace non-space character with a visually or acoustically similar character\n",
    "                \n",
    "                modified_char = get_similar_char(char)\n",
    "                modified_word += modified_char\n",
    "            else:\n",
    "                modified_word += char\n",
    "        modified_sentence.append(modified_word)\n",
    "    return modified_sentence\n",
    "\n",
    "def get_similar_char(char):\n",
    "    \"\"\"\n",
    "\n",
    "    :param char:\n",
    "    :return: modified characters based on similar_chars dict\n",
    "    \"\"\"\n",
    "    \n",
    "    similar_chars = {'a': 'e', 'b': 'd', 'c': 'e', 'd': 'b', 'e': 'a', 'f': 'g',\n",
    "                     'g': 'f', 'h': 'i', 'i': 'h', 'j': 'k', 'k': 'j', 'l': 'm',\n",
    "                     'm': 'l', 'n': 'o', 'o': 'n', 'p': 'q', 'q': 'p', 'r': 's',\n",
    "                     's': 'r', 't': 'u', 'u': 't', 'v': 'w', 'w': 'v', 'x': 'y',\n",
    "                     'y': 'x', 'z': 'z'}\n",
    "    return similar_chars.get(char, char)\n",
    "\n",
    "def modify_corpus(corpus, probability):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param corpus: \n",
    "    :param probability: \n",
    "    :return: falsy modified corpus\n",
    "    \"\"\"\n",
    "    modified_corpus = []\n",
    "    for sentence in corpus:\n",
    "        modified_sentence = replace_characters(sentence, probability)\n",
    "        modified_corpus.append(modified_sentence)\n",
    "    return modified_corpus\n",
    "\n",
    "# Example usage with a probability of 0.1 (10% chance of replacing each non-space character)\n",
    "modified_test_corpus = modify_corpus(cleaned_test_sentences, 0.1)\n",
    "\n",
    "print_cnt = 0\n",
    "for sent in modified_test_corpus:\n",
    "  print_cnt +=1\n",
    "  print(sent)\n",
    "  print(\"______________________\")\n",
    "  if print_cnt == 20:\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6YKZzyTkdJej",
    "outputId": "46afb53f-be2c-4ba5-bbcf-0543be1498b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test Corpus:\n",
      "[['Robert', 'D', '.', 'Hunter', ',', 'Chase', 'Manhattan', 'area', 'executive', 'for', 'Europe', ',', 'Africa', 'and', 'the', 'Middle', 'East', ',', 'said', 'at', 'a', 'news', 'conference', 'that', 'plans', 'to', 'broaden', 'the', 'bank', \"'\", 's', 'activities', 'on', 'the', 'Italian', 'market', 'have', 'not', 'been', '<UNK>', ',', 'however', '.'], ['Mr.', '<UNK>', 'gave', 'the', 'slow', 'movement', 'some', 'of', 'the', 'quality', 'of', 'a', '<UNK>', '<UNK>', '.'], ['From', 'time', 'to', 'time', 'the', 'medium', '<UNK>', 'other', 'people', '``', 'around', 'him', \"''\", ',', 'who', 'were', '``', 'on', 'the', 'other', 'side', \"''\", ',', 'and', 'reports', 'what', 'they', 'are', 'saying', '.'], ['<UNK>', 'blijkt', 'namelijk', 'dat', 'de', '<UNK>', 'niet', '<UNK>', 'zijn', 'een', '<UNK>', 'van', 'flo', '230', 'per', 'week', 'te', '<UNK>', '.'], ['Overall', ',', 'however', ',', 'trader', 'and', '<UNK>', 'opinion', 'on', 'the', 'likelihood', 'of', 'a', 'successful', 're', '-', 'negotiation', 'was', 'mildly', 'pessimistic', ',', 'varying', 'between', 'a', '50', '-', '50', 'chance', 'and', '60', '-', '40', 'against', '.'], ['``', 'An', 'active', '<UNK>', 'in', '<UNK>', '<UNK>', 'may', 'care', 'more', 'about', 'Senior', '<UNK>', 'games', ',', 'while', 'a', '<UNK>', 'in', '<UNK>', 'City', 'may', 'care', 'more', 'about', 'a', '<UNK>', 'on', 'health', ',', \"''\", 'she', 'says', '*T*-1', '.'], ['19', ':', '13', 'Thine', 'eye', 'shall', 'not', 'pity', 'him', ',', 'but', 'thou', 'shalt', 'put', 'away', 'the', 'guilt', 'of', 'innocent', 'blood', 'from', 'Israel', ',', 'that', 'it', 'may', 'go', 'well', 'with', 'thee', '.'], ['<UNK>', 'said', 'the', 'TransAmerican', '<UNK>', 'would', 'repay', 'its', '770', 'mln', 'dlr', 'debt', 'with', 'profits', 'from', 'natural', 'gas', 'sales', '.'], ['Also', 'seen', 'arriving', 'were', 'Japanese', 'Finance', 'Minister', 'Kiichi', 'Miyazawa', 'and', 'Japan', \"'\", 's', 'central', 'bank', 'governor', 'Satoshi', 'Sumita', 'and', 'British', 'Chancellor', 'of', 'the', 'Exchequer', 'and', 'central', 'bank', 'governor', 'Robin', 'Leigh', 'Pemberton', '.'], ['Armed', 'Forces', 'Day', '<UNK>', 'by', 'John', 'A.', 'Notte', ',', 'Jr.', ',', 'Governor'], ['A', 'blank', 'silence', 'followed', 'this', 'gay', 'inquiry', '.'], ['<UNK>', '<UNK>', 'should', 'be', 'distinguished', 'from', 'its', 'exact', 'opposite', '--', 'the', '<UNK>', ',', '<UNK>', 'invention', 'of', 'the', '<UNK>', '<UNK>', ',', 'or', 'present', '<UNK>', 'of', 'poetic', 'professors', '.'], ['In', '1986', 'the', 'company', 'earned', '1', '.', '75', 'dlrs', 'per', 'share', '.'], ['This', 'was', 'the', 'letter', 'which', 'would', 'or', 'would', 'not', '<UNK>', 'his', 'son', ',', 'David', ',', 'in', 'Hanford', '.'], ['But', 'the', 'U', '.', 'S', '.', 'Embassy', 'replied', 'that', 'Chinese', 'figures', 'showing', '13', 'years', 'of', 'trade', 'deficits', 'with', 'the', 'U', '.', 'S', '.', 'Out', 'of', 'the', 'last', '15', 'are', 'inaccurate', 'and', 'said', 'Peking', 'itself', 'would', 'have', 'to', 'persuade', 'Congress', 'to', 'change', 'laws', 'which', 'limit', 'its', 'exports', '.'], ['The', '<UNK>', 'newspaper', 'today', 'quoted', 'AT', 'and', 'T', 'chairman', 'James', 'E', '.', 'Olson', 'as', 'saying', 'that', 'Siemens', \"'\", 'attempt', 'to', 'stop', 'AT', 'and', 'T', 'and', 'Philips', 'could', 'lead', 'to', 'a', '<UNK>', 'of', 'protectionism', 'in', 'the', 'U', '.', 'S', '.', '<UNK>', 'said', 'he', 'expected', 'the', 'French', 'government', 'to', 'decide', 'on', 'the', 'winning', 'bid', 'by', 'the', 'end', 'of', 'April', '.'], ['they', 'will', 'hear', 'you', '.\"'], ['And', 'Isaac', 'departed', 'thence', ',', 'and', 'pitched', 'his', 'tent', 'in', 'the', 'valley', 'of', 'Gerar', ',', 'and', 'dwelt', 'there', '.'], ['And', 'it', 'came', 'to', 'pass', 'in', 'the', 'month', '<UNK>', ',', 'in', 'the', 'twentieth', 'year', ',', 'as', 'I', 'was', 'in', 'Shushan', 'the', 'palace', ',', '1', ':', '2', 'That', '<UNK>', ',', 'one', 'of', 'my', 'brethren', ',', 'came', ',', 'he', 'and', 'certain', 'men', 'of', 'Judah', ';', 'and', 'I', 'asked', 'them', 'concerning', 'the', 'Jews', 'that', 'had', 'escaped', ',', 'which', 'were', 'left', 'of', 'the', 'captivity', ',', 'and', 'concerning', 'Jerusalem', '.'], ['Article', '6', ',', 'entry', 'into', 'force'], ['Old', '<UNK>', '-', 'stand', 'gone', '.'], ['1', ':', '1', 'And', 'Solomon', 'the', 'son', 'of', 'David', 'was', 'strengthened', 'in', 'his', 'kingdom', ',', 'and', 'the', 'LORD', 'his', 'God', 'was', 'with', 'him', ',', 'and', 'magnified', 'him', 'exceedingly', '.'], ['\"', 'And', 'now', 'that', 'I', 'understand', 'your', 'question', ',', 'I', 'must', 'pronounce', 'it', 'to', 'be', 'a', 'very', 'unfair', 'one', '.'], ['BRAZILIAN', 'COFFEE', '<UNK>', 'THE', '<UNK>', '<UNK>', 'WAS', '<UNK>', 'IN', 'THE', '<UNK>', 'OVER', 'THE', '<UNK>', '24', '<UNK>', '<UNK>', 'STATE', ':', '<UNK>', 'NIL', ',', '<UNK>', 'NIL', ',', '<UNK>', 'NIL', ',', '<UNK>', 'NIL', '.'], ['The', 'crowd', 'at', 'the', '<UNK>', 'annual', 'K.', 'of', 'C.', '<UNK>', ',', 'final', '<UNK>', 'meet', 'of', 'the', 'season', ',', 'got', 'a', '<UNK>', 'a', 'few', 'minutes', 'earlier', 'when', 'a', 'slender', ',', '<UNK>', 'woman', 'broke', 'the', '<UNK>', 'world', 'record', 'in', 'the', '<UNK>', 'run', '.'], ['Thus', 'speculated', 'the', 'chief', 'of', 'a', 'party', 'upon', 'his', 'sleeping', 'friends', '.'], ['De', '<UNK>', 'is', 'dat', 'voor', 'Greene', 'de', '<UNK>', 'katholieke', '<UNK>', 'alleen', 'een', '<UNK>', 'is', 'geweest', ',', 'van', '<UNK>', 'hij', 'zich', 'heeft', 'weten', 'te', '<UNK>', 'tot', 'een', '<UNK>', '<UNK>', 'die', 'aan', 'geen', 'enkele', '<UNK>', 'is', '<UNK>', '.'], ['``', 'Yeah', \"''\", '.'], ['He', 'was', 'born', 'in', '<UNK>', ',', '<UNK>', ',', 'and', 'was', 'a', 'veteran', 'of', 'World', 'War', '1', '.'], ['Last', 'week', ',', 'dealers', 'said', 'the', 'Federal', 'Reserve', 'intervened', 'to', 'stop', 'the', 'dollar', 'rising', 'against', 'the', 'mark', ',', 'which', 'had', 'breached', '1', '.', '86', 'to', 'the', 'dollar', '.'], ['The', 'bushes', 'broke', 'and', 'snapped', 'abruptly', 'behind', 'them', ',', 'and', 'a', 'very', 'tall', 'figure', '<UNK>', 'above', 'Turnbull', 'with', 'an', '<UNK>', 'stoop', 'and', 'a', 'projecting', 'chin', ',', 'a', 'chin', 'of', 'which', 'the', 'shape', 'showed', '<UNK>', 'even', 'in', 'its', 'shadow', 'upon', 'the', 'path', '.'], ['Då', '<UNK>', 'Abimelek', ',', 'konungen', 'i', 'Gerar', ',', 'och', 'lät', '<UNK>', 'Sara', 'till', 'sig', '.'], ['Great', 'chapters', 'of', 'history', 'have', 'been', 'recorded', 'along', 'the', 'avenue', ',', 'now', 'about', '169', 'years', 'old', '.'], ['\"', 'What', 'is', 'it', 'my', 'dear', 'friend', '?'], ['When', 'she', 'thought', 'about', 'the', 'book', 'she', 'remembered', 'the', 'good', 'times', 'she', 'used', 'to', 'have', 'with', 'her', 'sisters', 'in', 'the', 'big', ',', 'bare', 'house', 'in', 'the', 'country', '.'], ['Reverend', '<UNK>', 'was', '<UNK>', 'bitter', '.'], ['He', 'stood', 'watching', 'the', 'girl', ',', 'wondering', 'what', 'was', 'coming', 'next', '.'], ['What', 'was', 'Poland', 'to', 'the', '<UNK>', '?'], ['and', 'Small', 'Business', '<UNK>', 'are', 'available', ',', 'on', 'request', ',', 'from', 'Small', 'Business', 'Administration', ',', 'Washington', '25', ',', 'D.C.', ',', 'and', 'its', 'regional', 'offices', '.'], ['a', '<UNK>', 'of', 'muscle', '``', 'drama', \"''\", 'that', 'is', '<UNK>', 'to', 'judges', 'and', 'audiences', 'alike', ';', ';'], ['\"', 'Yes', ',\"', 'said', 'Syme', ',', 'quite', 'motionless', ',', '\"', 'what', 'is', 'he', '?\"'], ['2', ':', '31', 'And', 'the', 'king', 'said', 'unto', 'him', ',', 'Do', 'as', 'he', 'hath', 'said', ',', 'and', 'fall', 'upon', 'him', ',', 'and', 'bury', 'him', ';', 'that', 'thou', 'mayest', 'take', 'away', 'the', 'innocent', 'blood', ',', 'which', 'Joab', 'shed', ',', 'from', 'me', ',', 'and', 'from', 'the', 'house', 'of', 'my', 'father', '.'], ['Deze', 'zaken', 'kunnen', 'alleen', 'uit', 'de', 'wereld', 'worden', '<UNK>', 'door', 'een', 'steeds', 'grotere', '<UNK>', '.'], ['Not', 'that', 'it', 'was', 'any', 'of', 'her', 'business', '.'], ['He', 'looked', 'down', 'for', 'a', 'moment', 'modestly', '.'], ['<UNK>', 'UNIT', 'RAISES', 'CRUDE', 'OIL', 'POSTINGS', '50', 'CTS', '/', 'BBL', ',', 'EFFECTIVE', 'YESTERDAY'], ['and', 'the', '<UNK>', 'frost', 'of', 'heaven', ',', 'who', 'hath', '<UNK>', 'it', '?'], ['এক', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '৷'], ['We', 'have', 'been', 'using', 'the', 'word', '``', 'public', \"''\", 'in', '<UNK>', 'marks', ',', 'that', 'is', ',', 'in', 'its', '<UNK>', '<UNK>', 'with', 'reference', 'to', 'the', 'odd-lot', 'index', 'theory', '.'], ['Lord', 'J', '.']]\n",
      "\n",
      "Modified Test Corpus:\n",
      "[['Robert', 'D', '.', 'Huntes', ',', 'Chase', 'Manhattan', 'area', 'axecutive', 'for', 'Europe', ',', 'Africa', 'and', 'the', 'Middle', 'Eest', ',', 'said', 'at', 'a', 'news', 'conferenca', 'thet', 'qlans', 'to', 'broaden', 'uhe', 'bank', \"'\", 's', 'activitier', 'on', 'uha', 'Itelian', 'merket', 'iave', 'not', 'been', '<UNK>', ',', 'hovaver', '.'], ['Mr.', '<UNK>', 'gave', 'uhe', 'slow', 'movemeot', 'some', 'og', 'the', 'quality', 'nf', 'a', '<UNK>', '<UNK>', '.'], ['From', 'thme', 'to', 'time', 'the', 'medium', '<UNK>', 'othas', 'people', '``', 'around', 'him', \"''\", ',', 'who', 'wera', '``', 'on', 'the', 'other', 'side', \"''\", ',', 'anb', 'raports', 'wiet', 'they', 'ara', 'saying', '.'], ['<UNK>', 'blikkt', 'namelikj', 'dat', 'de', '<UNK>', 'nieu', '<UNK>', 'zijn', 'een', '<UNK>', 'vao', 'flo', '230', 'per', 'week', 'ta', '<UNK>', '.'], ['Owerall', ',', 'hovever', ',', 'trader', 'end', '<UNK>', 'opinion', 'nn', 'the', 'likelihonb', 'of', 'a', 'successftl', 're', '-', 'oegotiation', 'was', 'mildly', 'pessimistie', ',', 'varyhng', 'detweeo', 'a', '50', '-', '50', 'chance', 'and', '60', '-', '40', 'against', '.'], ['``', 'An', 'actiwe', '<UNK>', 'in', '<UNK>', '<UNK>', 'may', 'care', 'mose', 'abotu', 'Sanior', '<UNK>', 'games', ',', 'whila', 'a', '<UNK>', 'in', '<UNK>', 'City', 'lay', 'case', 'lore', 'abouu', 'a', '<UNK>', 'on', 'health', ',', \"''\", 'she', 'says', '*T*-1', '.'], ['19', ':', '13', 'Thine', 'exe', 'siall', 'nou', 'pity', 'him', ',', 'buu', 'thou', 'shelt', 'put', 'eway', 'the', 'guilt', 'nf', 'innocent', 'blnod', 'frnl', 'Issael', ',', 'that', 'it', 'may', 'go', 'well', 'vith', 'tiee', '.'], ['<UNK>', 'said', 'tie', 'TransAmerican', '<UNK>', 'would', 'seqax', 'its', '770', 'mln', 'dlr', 'debt', 'witi', 'profius', 'from', 'neturam', 'gas', 'sales', '.'], ['Also', 'reen', 'arrhvhog', 'were', 'Japanese', 'Finance', 'Mioister', 'Kiicii', 'Miyazawa', 'and', 'Japan', \"'\", 's', 'centram', 'benk', 'governor', 'Satoshi', 'Sumita', 'and', 'British', 'Chancellor', 'of', 'the', 'Excheques', 'aod', 'centsel', 'baok', 'governor', 'Rnbin', 'Laigh', 'Pamberton', '.'], ['Armed', 'Forces', 'Day', '<UNK>', 'bx', 'John', 'A.', 'Noute', ',', 'Jr.', ',', 'Governor'], ['A', 'blank', 'silenca', 'folmowed', 'this', 'fey', 'inquiry', '.'], ['<UNK>', '<UNK>', 'should', 'be', 'distinguhshed', 'from', 'hts', 'exact', 'opposite', '--', 'the', '<UNK>', ',', '<UNK>', 'inwention', 'of', 'the', '<UNK>', '<UNK>', ',', 'nr', 'presenu', '<UNK>', 'of', 'poetic', 'professors', '.'], ['Io', '1986', 'the', 'company', 'earned', '1', '.', '75', 'blrs', 'per', 'siare', '.'], ['This', 'vas', 'the', 'letter', 'which', 'woulb', 'os', 'woumd', 'not', '<UNK>', 'his', 'son', ',', 'David', ',', 'in', 'Hanfnrd', '.'], ['But', 'tie', 'U', '.', 'S', '.', 'Embassy', 'replieb', 'thet', 'Chinese', 'fifures', 'showing', '13', 'yeass', 'of', 'trade', 'deficits', 'wiui', 'the', 'U', '.', 'S', '.', 'Ott', 'of', 'tha', 'last', '15', 'ere', 'ineccurate', 'and', 'said', 'Peking', 'itself', 'vould', 'have', 'tn', 'persuade', 'Congress', 'tn', 'change', 'laws', 'which', 'limit', 'ius', 'eyposus', '.'], ['The', '<UNK>', 'oewsqaper', 'today', 'qtoted', 'AT', 'and', 'T', 'chairman', 'James', 'E', '.', 'Olson', 'as', 'saying', 'that', 'Siemenr', \"'\", 'attempt', 'uo', 'stop', 'AT', 'and', 'T', 'and', 'Philips', 'could', 'lead', 'to', 'e', '<UNK>', 'of', 'protactionism', 'in', 'the', 'U', '.', 'S', '.', '<UNK>', 'raid', 'he', 'expected', 'the', 'French', 'government', 'to', 'decide', 'on', 'tie', 'winning', 'did', 'by', 'the', 'end', 'og', 'April', '.'], ['thax', 'vill', 'haer', 'you', '.\"'], ['Aod', 'Isaac', 'departed', 'thenea', ',', 'aod', 'pitched', 'his', 'tent', 'in', 'the', 'valley', 'nf', 'Gerar', ',', 'end', 'dwelt', 'there', '.'], ['Anb', 'it', 'cale', 'to', 'pass', 'in', 'the', 'month', '<UNK>', ',', 'io', 'tie', 'twentieth', 'year', ',', 'ar', 'I', 'war', 'in', 'Shushan', 'tie', 'pamace', ',', '1', ':', '2', 'That', '<UNK>', ',', 'ona', 'of', 'mx', 'brethren', ',', 'came', ',', 'he', 'and', 'cartain', 'len', 'of', 'Judah', ';', 'and', 'I', 'arked', 'thel', 'cnncerning', 'the', 'Jews', 'that', 'had', 'escaqed', ',', 'wiich', 'were', 'left', 'ng', 'the', 'capuhvity', ',', 'anb', 'concerning', 'Jertsalem', '.'], ['Artiele', '6', ',', 'entry', 'into', 'forca'], ['Old', '<UNK>', '-', 'stanb', 'gone', '.'], ['1', ':', '1', 'And', 'Solomon', 'the', 'soo', 'of', 'Davhd', 'was', 'strengthenad', 'in', 'his', 'kingdnm', ',', 'and', 'tha', 'LORD', 'his', 'Gnd', 'was', 'with', 'him', ',', 'and', 'magnified', 'him', 'exceedingly', '.'], ['\"', 'Anb', 'now', 'thau', 'I', 'understand', 'yotr', 'puestion', ',', 'I', 'must', 'pronounce', 'it', 'tn', 'be', 'a', 'vesy', 'unfahr', 'one', '.'], ['BRAZILIAN', 'COFFEE', '<UNK>', 'THE', '<UNK>', '<UNK>', 'WAS', '<UNK>', 'IN', 'THE', '<UNK>', 'OVER', 'THE', '<UNK>', '24', '<UNK>', '<UNK>', 'STATE', ':', '<UNK>', 'NIL', ',', '<UNK>', 'NIL', ',', '<UNK>', 'NIL', ',', '<UNK>', 'NIL', '.'], ['The', 'crnwd', 'at', 'tie', '<UNK>', 'annual', 'K.', 'of', 'C.', '<UNK>', ',', 'ginal', '<UNK>', 'meet', 'of', 'the', 'seasnn', ',', 'gou', 'a', '<UNK>', 'a', 'few', 'mhnutes', 'earlies', 'when', 'e', 'rlender', ',', '<UNK>', 'woman', 'bsoje', 'the', '<UNK>', 'world', 'record', 'io', 'the', '<UNK>', 'run', '.'], ['Thus', 'spacumatad', 'tie', 'ehief', 'of', 'a', 'pasty', 'upon', 'his', 'rleeping', 'fsiends', '.'], ['De', '<UNK>', 'is', 'dat', 'vnor', 'Greeoe', 'de', '<UNK>', 'jatholieke', '<UNK>', 'amleen', 'een', '<UNK>', 'is', 'geweest', ',', 'van', '<UNK>', 'hij', 'zich', 'heefu', 'weten', 'te', '<UNK>', 'tot', 'een', '<UNK>', '<UNK>', 'die', 'aan', 'geeo', 'eokele', '<UNK>', 'is', '<UNK>', '.'], ['``', 'Yeah', \"''\", '.'], ['He', 'wes', 'boro', 'in', '<UNK>', ',', '<UNK>', ',', 'end', 'war', 'e', 'veteran', 'of', 'World', 'Was', '1', '.'], ['Last', 'weak', ',', 'dealers', 'raid', 'the', 'Federal', 'Reserve', 'iotasvened', 'to', 'stnp', 'the', 'bnllar', 'rising', 'egainst', 'the', 'lark', ',', 'which', 'had', 'breached', '1', '.', '86', 'to', 'the', 'dollar', '.'], ['The', 'busies', 'broke', 'anb', 'rnapped', 'abruptly', 'behind', 'uhem', ',', 'and', 'a', 'vesy', 'tall', 'gigure', '<UNK>', 'above', 'Turnbull', 'with', 'an', '<UNK>', 'suoop', 'and', 'a', 'psojecting', 'chin', ',', 'a', 'chin', 'of', 'whieh', 'the', 'shape', 'sinwed', '<UNK>', 'even', 'in', 'hts', 'shadow', 'upon', 'tha', 'path', '.'], ['Då', '<UNK>', 'Abimelek', ',', 'konunfen', 'i', 'Gerar', ',', 'och', 'lät', '<UNK>', 'Sara', 'till', 'rig', '.'], ['Great', 'chapterr', 'of', 'histnry', 'have', 'been', 'recordeb', 'alonf', 'the', 'avenue', ',', 'now', 'abouu', '169', 'xears', 'omd', '.'], ['\"', 'What', 'is', 'it', 'ly', 'baar', 'friand', '?'], ['When', 'she', 'uhought', 'abott', 'tha', 'book', 'she', 'remembered', 'uhe', 'goob', 'times', 'sie', 'used', 'to', 'have', 'with', 'her', 'sisters', 'in', 'the', 'big', ',', 'bare', 'house', 'in', 'the', 'country', '.'], ['Revereod', '<UNK>', 'wes', '<UNK>', 'biuter', '.'], ['He', 'stood', 'wetching', 'the', 'girl', ',', 'vondering', 'viat', 'was', 'coming', 'next', '.'], ['What', 'was', 'Pnland', 'to', 'the', '<UNK>', '?'], ['and', 'Small', 'Business', '<UNK>', 'are', 'availeble', ',', 'oo', 'sequest', ',', 'grom', 'Small', 'Business', 'Ablinistration', ',', 'Weshington', '25', ',', 'D.C.', ',', 'and', 'ius', 'refional', 'offices', '.'], ['a', '<UNK>', 'of', 'muscla', '``', 'drema', \"''\", 'that', 'is', '<UNK>', 'to', 'judges', 'and', 'audiences', 'alike', ';', ';'], ['\"', 'Yes', ',\"', 'said', 'Syme', ',', 'puite', 'mntionmess', ',', '\"', 'what', 'is', 'he', '?\"'], ['2', ':', '31', 'And', 'the', 'khng', 'said', 'unto', 'him', ',', 'Do', 'es', 'ha', 'hath', 'raid', ',', 'and', 'fall', 'upon', 'him', ',', 'enb', 'btry', 'him', ';', 'that', 'uhnu', 'mayest', 'take', 'away', 'the', 'innocenu', 'bmood', ',', 'whieh', 'Joab', 'shed', ',', 'frnm', 'ma', ',', 'end', 'from', 'the', 'house', 'of', 'my', 'father', '.'], ['Deze', 'zaken', 'kunnen', 'almeen', 'uit', 'de', 'weremd', 'vorden', '<UNK>', 'dnns', 'eeo', 'steeds', 'grotera', '<UNK>', '.'], ['Not', 'that', 'it', 'was', 'any', 'of', 'her', 'business', '.'], ['Ha', 'looked', 'down', 'for', 'e', 'momenu', 'modestly', '.'], ['<UNK>', 'UNIT', 'RAISES', 'CRUDE', 'OIL', 'POSTINGS', '50', 'CTS', '/', 'BBL', ',', 'EFFECTIVE', 'YESTERDAY'], ['and', 'the', '<UNK>', 'frosu', 'of', 'heeven', ',', 'who', 'hati', '<UNK>', 'iu', '?'], ['এক', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '৷'], ['We', 'have', 'bean', 'using', 'uha', 'wosd', '``', 'public', \"''\", 'in', '<UNK>', 'marks', ',', 'that', 'is', ',', 'in', 'itr', '<UNK>', '<UNK>', 'with', 'raferenca', 'to', 'the', 'odd-lot', 'indey', 'theory', '.'], ['Lord', 'J', '.']]\n",
      "\n",
      "Corrected Test Corpus:\n",
      "[['Robert', 'D', '.', 'Hunter', ',', 'Chase', 'Manhattan', 'area', 'executive', 'for', 'Europe', ',', 'Africa', 'and', 'the', 'Middle', 'rest', ',', 'said', 'at', 'a', 'news', 'conference', 'the', 'plans', 'to', 'broaden', 'The', 'bank', \"'\", 's', 'activities', 'on', 'uma', 'Italian', 'market', 'have', 'not', 'been', '<UNK>', ',', 'however', '.'], ['Mr.', '<UNK>', 'gave', 'The', 'slow', 'movement', 'some', 'of', 'the', 'quality', 'of', 'a', '<UNK>', '<UNK>', '.'], ['From', 'the', 'to', 'time', 'the', 'medium', '<UNK>', 'that', 'people', '``', 'around', 'him', \"''\", ',', 'who', 'were', '``', 'on', 'the', 'other', 'side', \"''\", ',', 'and', 'reports', 'niet', 'they', 'are', 'saying', '.'], ['<UNK>', 'blijkt', 'namelijk', 'dat', 'de', '<UNK>', 'niet', '<UNK>', 'zijn', 'een', '<UNK>', 'van', 'flo', '230', 'per', 'week', 'ta', '<UNK>', '.'], ['Overall', ',', 'however', ',', 'trader', 'end', '<UNK>', 'opinion', 'in', 'the', 'likelihood', 'of', 'a', 'successful', 're', '-', 'negotiation', 'was', 'mildly', 'pessimistic', ',', 'varying', 'between', 'a', '50', '-', '50', 'chance', 'and', '60', '-', '40', 'against', '.'], ['``', 'An', 'active', '<UNK>', 'in', '<UNK>', '<UNK>', 'may', 'care', 'more', 'about', 'Senior', '<UNK>', 'games', ',', 'while', 'a', '<UNK>', 'in', '<UNK>', 'City', 'lay', 'case', 'lore', 'about', 'a', '<UNK>', 'on', 'health', ',', \"''\", 'she', 'says', '*T*-1', '.'], ['19', ':', '13', 'Thine', 'eye', 'shall', 'no', 'pity', 'him', ',', 'but', 'thou', 'shalt', 'put', 'away', 'the', 'guilt', 'of', 'innocent', 'blood', 'from', 'Israel', ',', 'that', 'it', 'may', 'go', 'well', 'with', 'thee', '.'], ['<UNK>', 'said', 'tie', 'TransAmerican', '<UNK>', 'would', 'sea', 'its', '770', 'mln', 'dlr', 'debt', 'with', 'profits', 'from', 'neutral', 'gas', 'sales', '.'], ['Also', 'been', 'arriving', 'were', 'Japanese', 'Finance', 'Minister', 'Kiichi', 'Miyazawa', 'and', 'Japan', \"'\", 's', 'central', 'bank', 'governor', 'Satoshi', 'Sumita', 'and', 'British', 'Chancellor', 'of', 'the', 'Exchequer', 'and', 'central', 'back', 'governor', 'Robin', 'Leigh', 'Pemberton', '.'], ['Armed', 'Forces', 'Day', '<UNK>', 'be', 'John', 'A.', 'Note', ',', 'Jr.', ',', 'Governor'], ['A', 'blank', 'silence', 'followed', 'this', 'few', 'inquiry', '.'], ['<UNK>', '<UNK>', 'should', 'be', 'distinguished', 'from', 'its', 'exact', 'opposite', '--', 'the', '<UNK>', ',', '<UNK>', 'invention', 'of', 'the', '<UNK>', '<UNK>', ',', 'no', 'present', '<UNK>', 'of', 'poetic', 'professors', '.'], ['In', '1986', 'the', 'company', 'earned', '1', '.', '75', 'dlrs', 'per', 'share', '.'], ['This', 'mas', 'the', 'letter', 'which', 'would', 'os', 'would', 'not', '<UNK>', 'his', 'son', ',', 'David', ',', 'in', 'Hanford', '.'], ['But', 'tie', 'U', '.', 'S', '.', 'Embassy', 'replied', 'the', 'Chinese', 'figures', 'showing', '13', 'years', 'of', 'trade', 'deficits', 'will', 'the', 'U', '.', 'S', '.', 'Out', 'of', 'the', 'last', '15', 'ere', 'inaccurate', 'and', 'said', 'Peking', 'itself', 'would', 'have', 'in', 'persuade', 'Congress', 'in', 'change', 'laws', 'which', 'limit', 'its', 'exports', '.'], ['The', '<UNK>', 'newspaper', 'today', 'quoted', 'AT', 'and', 'T', 'chairman', 'James', 'E', '.', 'Olson', 'as', 'saying', 'that', 'Siemens', \"'\", 'attempt', 'to', 'stop', 'AT', 'and', 'T', 'and', 'Philips', 'could', 'lead', 'to', 'e', '<UNK>', 'of', 'protectionism', 'in', 'the', 'U', '.', 'S', '.', '<UNK>', 'raid', 'he', 'expected', 'the', 'French', 'government', 'to', 'decide', 'on', 'tie', 'winning', 'did', 'by', 'the', 'end', 'of', 'April', '.'], ['that', 'vill', 'her', 'you', '.\"'], ['God', 'Isaac', 'departed', 'thee', ',', 'and', 'pitched', 'his', 'tent', 'in', 'the', 'valley', 'of', 'Gerar', ',', 'end', 'dwelt', 'there', '.'], ['And', 'it', 'case', 'to', 'pass', 'in', 'the', 'month', '<UNK>', ',', 'in', 'tie', 'twentieth', 'year', ',', 'a', 'I', 'war', 'in', 'Shushan', 'tie', 'palace', ',', '1', ':', '2', 'That', '<UNK>', ',', 'on', 'of', 'me', 'brethren', ',', 'came', ',', 'he', 'and', 'certain', 'leg', 'of', 'Judah', ';', 'and', 'I', 'arken', 'the', 'concerning', 'the', 'Jews', 'that', 'had', 'escaped', ',', 'which', 'were', 'left', 'no', 'the', 'captivity', ',', 'and', 'concerning', 'Jerusalem', '.'], ['Article', '6', ',', 'entry', 'into', 'force'], ['Old', '<UNK>', '-', 'stand', 'gone', '.'], ['1', ':', '1', 'And', 'Solomon', 'the', 'so', 'of', 'David', 'was', 'strengthened', 'in', 'his', 'kingdom', ',', 'and', 'the', 'LORD', 'his', 'and', 'was', 'with', 'him', ',', 'and', 'magnified', 'him', 'exceedingly', '.'], ['\"', 'And', 'now', 'that', 'I', 'understand', 'your', 'question', ',', 'I', 'must', 'pronounce', 'it', 'in', 'be', 'a', 'very', 'unfair', 'one', '.'], ['BRAZILIAN', 'COFFEE', '<UNK>', 'THE', '<UNK>', '<UNK>', 'WAS', '<UNK>', 'IN', 'THE', '<UNK>', 'OVER', 'THE', '<UNK>', '24', '<UNK>', '<UNK>', 'STATE', ':', '<UNK>', 'NIL', ',', '<UNK>', 'NIL', ',', '<UNK>', 'NIL', ',', '<UNK>', 'NIL', '.'], ['The', 'crowd', 'at', 'tie', '<UNK>', 'annual', 'K.', 'of', 'C.', '<UNK>', ',', 'final', '<UNK>', 'meet', 'of', 'the', 'season', ',', 'go', 'a', '<UNK>', 'a', 'few', 'minutes', 'earlier', 'when', 'e', 'slender', ',', '<UNK>', 'woman', 'some', 'the', '<UNK>', 'world', 'record', 'in', 'the', '<UNK>', 'run', '.'], ['Thus', 'speculated', 'tie', 'chief', 'of', 'a', 'past', 'upon', 'his', 'sleeping', 'friends', '.'], ['De', '<UNK>', 'is', 'dat', 'nor', 'Greece', 'de', '<UNK>', 'katholieke', '<UNK>', 'alleen', 'een', '<UNK>', 'is', 'geweest', ',', 'van', '<UNK>', 'hij', 'zich', 'heeft', 'weten', 'te', '<UNK>', 'tot', 'een', '<UNK>', '<UNK>', 'die', 'aan', 'geen', 'enkele', '<UNK>', 'is', '<UNK>', '.'], ['``', 'Yeah', \"''\", '.'], ['He', 'was', 'born', 'in', '<UNK>', ',', '<UNK>', ',', 'end', 'war', 'e', 'veteran', 'of', 'World', 'Was', '1', '.'], ['Last', 'weak', ',', 'dealers', 'raid', 'the', 'Federal', 'Reserve', 'intervened', 'to', 'step', 'the', 'dollar', 'rising', 'against', 'the', 'dark', ',', 'which', 'had', 'breached', '1', '.', '86', 'to', 'the', 'dollar', '.'], ['The', 'bushes', 'broke', 'and', 'snapped', 'abruptly', 'behind', 'them', ',', 'and', 'a', 'very', 'tall', 'figure', '<UNK>', 'above', 'Turnbull', 'with', 'an', '<UNK>', 'stoop', 'and', 'a', 'projecting', 'chin', ',', 'a', 'chin', 'of', 'which', 'the', 'shape', 'sinned', '<UNK>', 'even', 'in', 'its', 'shadow', 'upon', 'the', 'path', '.'], ['Då', '<UNK>', 'Abimelek', ',', 'konungen', 'i', 'Gerar', ',', 'och', 'lät', '<UNK>', 'Sara', 'till', 'rig', '.'], ['Great', 'chapter', 'of', 'history', 'have', 'been', 'recorded', 'alone', 'the', 'avenue', ',', 'now', 'about', '169', 'years', 'old', '.'], ['\"', 'What', 'is', 'it', 'lt', 'maar', 'friend', '?'], ['When', 'she', 'thought', 'about', 'the', 'book', 'she', 'remembered', 'The', 'good', 'times', 'sie', 'used', 'to', 'have', 'with', 'her', 'sisters', 'in', 'the', 'big', ',', 'bare', 'house', 'in', 'the', 'country', '.'], ['Reverend', '<UNK>', 'was', '<UNK>', 'bitter', '.'], ['He', 'stood', 'watching', 'the', 'girl', ',', 'wondering', 'vit', 'was', 'coming', 'next', '.'], ['What', 'was', 'inland', 'to', 'the', '<UNK>', '?'], ['and', 'Small', 'Business', '<UNK>', 'are', 'available', ',', 'of', 'request', ',', 'from', 'Small', 'Business', 'Administration', ',', 'Washington', '25', ',', 'D.C.', ',', 'and', 'its', 'regional', 'offices', '.'], ['a', '<UNK>', 'of', 'muscle', '``', 'drama', \"''\", 'that', 'is', '<UNK>', 'to', 'judges', 'and', 'audiences', 'alike', ';', ';'], ['\"', 'Yes', ',\"', 'said', 'Syme', ',', 'quite', 'motionless', ',', '\"', 'what', 'is', 'he', '?\"'], ['2', ':', '31', 'And', 'the', 'king', 'said', 'unto', 'him', ',', 'Do', 'es', 'ha', 'hath', 'raid', ',', 'and', 'fall', 'upon', 'him', ',', 'end', 'try', 'him', ';', 'that', 'thou', 'mayest', 'take', 'away', 'the', 'innocent', 'blood', ',', 'which', 'Joab', 'shed', ',', 'from', 'ma', ',', 'end', 'from', 'the', 'house', 'of', 'my', 'father', '.'], ['Deze', 'zaken', 'kunnen', 'alleen', 'uit', 'de', 'wereld', 'worden', '<UNK>', 'dans', 'een', 'steeds', 'grotere', '<UNK>', '.'], ['Not', 'that', 'it', 'was', 'any', 'of', 'her', 'business', '.'], ['Ha', 'looked', 'down', 'for', 'e', 'moment', 'modestly', '.'], ['<UNK>', 'UNIT', 'RAISES', 'CRUDE', 'OIL', 'POSTINGS', '50', 'CTS', '/', 'BBL', ',', 'EFFECTIVE', 'YESTERDAY'], ['and', 'the', '<UNK>', 'frost', 'of', 'heaven', ',', 'who', 'hath', '<UNK>', 'in', '?'], ['এক', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '৷'], ['We', 'have', 'been', 'using', 'uma', 'wood', '``', 'public', \"''\", 'in', '<UNK>', 'marks', ',', 'that', 'is', ',', 'in', 'its', '<UNK>', '<UNK>', 'with', 'reference', 'to', 'the', 'odd-lot', 'index', 'theory', '.'], ['Lord', 'J', '.']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Take a portion of the test_corpus and modified_test_corpus\n",
    "# Selecting all the test sentences might reach a bottleneck in cpu execution times!\n",
    "org_sent = cleaned_test_sentences[:50]\n",
    "wrg_sent = modified_test_corpus[:50]\n",
    "\n",
    "def correct_corpus_np(corpus, vocab, max_candidates=5):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    :param corpus:\n",
    "    :param vocab:\n",
    "    :param max_candidates:\n",
    "    :return: spell corrected corpus\n",
    "    \"\"\"\n",
    "    corrected_corpus = []\n",
    "    for sentence in corpus:\n",
    "        corrected_sentence = beam_search_spelling(sentence, 3,0,1, generate_candidate_with_distance, score)\n",
    "        corrected_corpus.append(corrected_sentence)\n",
    "    return corrected_corpus\n",
    "\n",
    "corrected_test_corpus = correct_corpus_np(wrg_sent, vocab, 5)\n",
    "\n",
    "print(\"Original Test Corpus:\")\n",
    "print(org_sent)\n",
    "\n",
    "print(\"\\nModified Test Corpus:\")\n",
    "print(wrg_sent)\n",
    "\n",
    "print(\"\\nCorrected Test Corpus:\")\n",
    "print(corrected_test_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlhtYwTZdO7X"
   },
   "source": [
    "## vi) Evaluate the context-aware spelling corrector in terms of Word Error Rate (WER) and Character Error Rate (CER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dpi-XeZIdWi8",
    "outputId": "4493c585-544d-40ce-b4a2-8b2771a9a2bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER score corrected vs original is: 0.08775137111517367\n",
      "WER score artificial vs original is: 0.2595978062157221\n",
      "CER score corrected vs original is: 0.026370314560180824\n",
      "CER score wrong vs original is: 0.06140516104727821\n"
     ]
    }
   ],
   "source": [
    "# !pip install evaluate\n",
    "# !pip install jiwer\n",
    "from evaluate import load\n",
    "\n",
    "cor_sent = corrected_test_corpus[:50]\n",
    "\n",
    "# Flatten the list of lists\n",
    "flattened_cor_sent = [' '.join(sentence) for sentence in cor_sent]\n",
    "flattened_org_sent = [' '.join(sentence) for sentence in org_sent]\n",
    "flattened_wrg_sent =  [' '.join(sentence) for sentence in wrg_sent]\n",
    "# Transform predictions\n",
    "predictions1 = [' '.join(flattened_cor_sent)]\n",
    "\n",
    "references = [' '.join(flattened_org_sent)]\n",
    "\n",
    "predictions2 = [' '.join(flattened_wrg_sent)]\n",
    "\n",
    "wer1 = load(\"wer\")  # Load Word-Error-Rate metric\n",
    "wer_score = wer1.compute(predictions=predictions1, references=references)\n",
    "print(f\"WER score corrected vs original is: {wer_score}\")\n",
    "\n",
    "\n",
    "wer2 = load(\"wer\")  # Load Word-Error-Rate metric\n",
    "wer_score = wer2.compute(predictions=predictions2, references=references)\n",
    "print(f\"WER score artificial vs original is: {wer_score}\")\n",
    "\n",
    "\n",
    "cer1 = load(\"cer\")\n",
    "cer_score = cer1.compute(predictions=predictions1, references=references)\n",
    "print(f\"CER score corrected vs original is: {cer_score}\")\n",
    "\n",
    "\n",
    "cer2 = load(\"cer\")\n",
    "cer_score = cer2.compute(predictions=predictions2, references=references)\n",
    "print(f\"CER score wrong vs original is: {cer_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
